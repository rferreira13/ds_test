{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"answers/","title":"Case","text":"<p>nome: Rafael Marinho Ferreira</p> <p>email: rafamarinho87@gmail.com</p> <p>github: https://github.com/rferreira13</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport scipy.stats as st\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.tsa.stattools import adfuller\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import make_scorer, recall_score\n</pre> import pandas as pd import numpy as np import scipy.stats as st import seaborn as sns import matplotlib.pyplot as plt  import statsmodels.api as sm import statsmodels.formula.api as smf from statsmodels.tsa.stattools import adfuller  from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier from sklearn.svm import SVC, LinearSVC from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from imblearn.over_sampling import RandomOverSampler from sklearn.model_selection import GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report, confusion_matrix, f1_score from sklearn.model_selection import train_test_split from sklearn.metrics import make_scorer, recall_score In\u00a0[2]: Copied! <pre>df = pd.read_excel(\"../Test O_G_Equipment_Data.xlsx\")\n</pre> df = pd.read_excel(\"../Test O_G_Equipment_Data.xlsx\") In\u00a0[3]: Copied! <pre>counts = df[\"Fail\"].value_counts(dropna=False).rename_axis(\"Fail\").reset_index(name=\"count\")\nfig, ax = plt.subplots(figsize=(8,6))\nbars = ax.bar(counts[\"Fail\"].astype(str), counts[\"count\"])\nax.bar_label(bars, padding=3)\nax.set_title(\"Count of Fail and Not Fail Events\", fontsize=26, pad=20, loc=\"center\")\nax.set_ylim(0, df.shape[0])\nfig.subplots_adjust(left=0.15, right=0.95, top=0.85, bottom=0.15)\nplt.show()\n</pre> counts = df[\"Fail\"].value_counts(dropna=False).rename_axis(\"Fail\").reset_index(name=\"count\") fig, ax = plt.subplots(figsize=(8,6)) bars = ax.bar(counts[\"Fail\"].astype(str), counts[\"count\"]) ax.bar_label(bars, padding=3) ax.set_title(\"Count of Fail and Not Fail Events\", fontsize=26, pad=20, loc=\"center\") ax.set_ylim(0, df.shape[0]) fig.subplots_adjust(left=0.15, right=0.95, top=0.85, bottom=0.15) plt.show() In\u00a0[223]: Copied! <pre>\"{:.2%}\".format(df['Fail'].mean())\n</pre> \"{:.2%}\".format(df['Fail'].mean()) Out[223]: <pre>'8.25%'</pre> In\u00a0[41]: Copied! <pre>def failure_report(data, group):\n    data_copy = data.copy()\n    total = (data_copy[group].astype(str)\n            .value_counts().rename_axis(group)\n            .reset_index(name='Number of observations'))\n    taxa = (\n        data_copy[data_copy[\"Fail\"]][group].astype(str).value_counts() \n        / data_copy[group].astype(str).value_counts()\n        ).reset_index(name='Failure percentage')\n    base = (total.merge(taxa, on=group, how='left')\n                .fillna({'Failure percentage': 0})\n                .sort_values(group))\n    fig, ax1 = plt.subplots(figsize=(8,6))\n    bars = ax1.bar(base[group].astype(str), base['Number of observations'], label='Number of observations')\n    ax1.bar_label(bars, padding=3)\n    ax1.set_ylabel('Number of observations')\n    ax1.set_xlabel(group)\n    ax2 = ax1.twinx()\n    ax2.plot(base[group].astype(str), base['Failure percentage'], marker='o', linewidth=2, color='red', label='Failure percentage')\n    for i, v in enumerate(base['Failure percentage']):\n        ax2.text(i, v, f'{v:.1%}', ha='center', va='bottom')\n    ax2.set_ylabel('Failure (%)')\n    ax1.set_title(f\"Number of observations versus Failure percentage\\nConsidering {group}\", fontsize=26, pad=20)\n    ticks = ax2.get_yticks()\n    ax2.set_yticklabels([f'{t:.1%}' for t in ticks])\n    fig.subplots_adjust(left=0.15, right=0.95, top=0.85, bottom=0.15)\n    plt.show()\n\ndef heatmap_failure(data):\n    data_copy = data.copy()\n    matrix_data = (data_copy.groupby(['Preset_1','Preset_2'])['Fail'].mean()\n        .mul(100).unstack(fill_value=0).sort_index())\n    matrix_data.columns = matrix_data.columns.astype(str)\n    matrix_data.index = matrix_data.index.astype(str)\n    fig, ax = plt.subplots(figsize=(8,6))\n    im = ax.imshow(matrix_data.values, cmap='Reds', aspect='auto')\n    ax.set_xticks(range(matrix_data.shape[1]))\n    ax.set_xticklabels(matrix_data.columns)\n    ax.set_yticks(range(matrix_data.shape[0]))\n    ax.set_yticklabels(matrix_data.index)\n    for i in range(matrix_data.shape[0]):\n        for j in range(matrix_data.shape[1]):\n            ax.text(j, i, f\"{matrix_data.values[i, j]:.1f}\", ha='center', va='center')\n    ax.set_xlabel('Preset_2')\n    ax.set_ylabel('Preset_1')\n    cbar = plt.colorbar(im)\n    cbar.set_label('Fail %')\n    ax.set_title(\"Percentage Failure Heatmap\\nCombining Preset_1 and Preset_2\", fontsize=26, pad=20)\n    fig.subplots_adjust(left=0.15, right=0.95, top=0.85, bottom=0.15)\n    plt.show()\n\ndef heatmap_configuration_changes(data, group):\n    data_copy = data.copy()\n    data_copy[f'Previous {group}'] = data_copy[group].shift()\n    data_copy = data_copy.dropna()\n    data_copy[f'Previous {group}'] = data_copy[f'Previous {group}'].astype(int)\n    data_copy[f'Previous {group}'] = data_copy[f'Previous {group}'].astype(str)\n    data_copy[group] = data_copy[group].astype(str)\n    n_changes_df = (\n        data_copy\n        [[f'Previous {group}', group]]\n        .value_counts()\n        .reset_index(name=\"Number of Changes\")\n    )\n    n_changes_df[\"Percent of changes during operation\"] = n_changes_df[\"Number of Changes\"] / data_copy.shape[0]\n    n_changes_df = (\n        n_changes_df\n        .rename(columns={\n            f'Previous {group}':\"From\",\n            group:\"To\"\n            }\n        )\n        .sort_values(by=[\"From\", \"To\"])\n        .reset_index(drop=True)\n    )\n    order_x = sorted(n_changes_df[\"From\"].unique())\n    order_y = sorted(n_changes_df[\"To\"].unique())\n    fig, ax = plt.subplots(figsize=(8,6))\n    z = np.zeros((len(order_y), len(order_x)))\n    for i, y in enumerate(order_y):\n        for j, x in enumerate(order_x):\n            v = n_changes_df.loc[(n_changes_df[\"From\"]==x) &amp; (n_changes_df[\"To\"]==y), \"Percent of changes during operation\"].values\n            z[i, j] = v[0] if len(v) else 0\n    vmax = z.max() if z.max() &gt; 0 else 1\n    vmin = z[z&gt;0].min() if np.any(z&gt;0) else 0\n    im = ax.imshow(z, cmap=\"Reds\", aspect=\"auto\", vmin=vmin, vmax=vmax)\n    ax.set_xticks(range(len(order_x)))\n    ax.set_xticklabels(order_x)\n    ax.set_yticks(range(len(order_y)))\n    ax.set_yticklabels(order_y)\n    for i in range(len(order_y)):\n        for j in range(len(order_x)):\n            ax.text(j, i, f\"{z[i,j]:.2%}\", ha=\"center\", va=\"center\")\n    ticks = np.linspace(vmin, vmax, 6)\n    cbar = plt.colorbar(im, ticks=ticks)\n    cbar.set_ticklabels([f\"{t:.0%}\" for t in ticks])\n    cbar.set_label(\"Percent of changes during operation\")\n    ax.set_title(f\"Percent of changes from {group} configurations during operation\", fontsize=26, pad=20)\n    fig.subplots_adjust(left=0.15, right=0.95, top=0.85, bottom=0.15)\n    if n_changes_df.shape[0] &gt; 200:\n        fig.set_figheight(9)\n        fig.set_figwidth(20)\n    plt.show()\n\n\n\n\ndef boxplots(data, group, variable):\n    data_copy = data.copy()\n    model = smf.ols(f'{variable} ~ C({group})', data=data_copy).fit()\n    resid = model.resid\n    fitted = model.fittedvalues\n    fig = st.probplot(resid, dist=\"norm\", plot=plt)\n    plt.title(f\"QQ plot of residuals - {variable} ~ C({group})\")\n    plt.show()\n    plt.scatter(fitted, resid, s=8, alpha=0.6)\n    plt.axhline(0, lw=1, color='k')\n    plt.xlabel(\"Fitted values\")\n    plt.ylabel(\"Residuals\")\n    plt.title(f\"Residuals vs Fitted - {variable} ~ C({group})\")\n    plt.show()\n    fig, ax = plt.subplots()\n    data_copy.boxplot(column=variable, by=group, ax=ax)\n    ax.set_xlabel(group)\n    ax.set_ylabel(variable)\n    plt.suptitle(\"\")\n    plt.xticks(rotation=45)\n    fig.subplots_adjust(top=0.85, bottom=0.2)\n    model = smf.ols(f'{variable} ~ C({group})', data=data_copy).fit()\n    anova_tbl = sm.stats.anova_lm(model, typ=2)\n    df1 = float(anova_tbl.loc[f'C({group})', 'df'])\n    df2 = float(anova_tbl.loc['Residual', 'df'])\n    Fval = float(anova_tbl.loc[f'C({group})', 'F'])\n    pval = float(anova_tbl.loc[f'C({group})', 'PR(&gt;F)'])\n    ss_effect = float(anova_tbl.loc[f'C({group})', 'sum_sq'])\n    ss_error  = float(anova_tbl.loc['Residual', 'sum_sq'])\n    mse = ss_error / df2\n    y = data_copy[variable].to_numpy(dtype=float)\n    ss_total = float(((y - y.mean())**2).sum())\n    eta2   = ss_effect / ss_total\n    omega2 = (ss_effect - df1 * mse) / (ss_total + mse)\n    k = data_copy[group].nunique()\n    N = len(data_copy)\n    text_ = (\n        f\"ANOVA (one-way) \u2014 {variable} ~ {group}\\n\"\n        f\"F({int(df1)}, {int(df2)}) = {Fval:.2f}, p = {pval:.3g} \"\n        f\"| \u03b7\u00b2 = {eta2:.3f}, \u03c9\u00b2 = {omega2:.3f} \"\n        f\"| k = {k}, N = {N}\"\n    )\n    fig.text(0.0, 0.98, text_, ha='left', va='top', bbox=dict(facecolor='white', alpha=0.75, edgecolor='black', linewidth=1))\n    plt.show()\n</pre> def failure_report(data, group):     data_copy = data.copy()     total = (data_copy[group].astype(str)             .value_counts().rename_axis(group)             .reset_index(name='Number of observations'))     taxa = (         data_copy[data_copy[\"Fail\"]][group].astype(str).value_counts()          / data_copy[group].astype(str).value_counts()         ).reset_index(name='Failure percentage')     base = (total.merge(taxa, on=group, how='left')                 .fillna({'Failure percentage': 0})                 .sort_values(group))     fig, ax1 = plt.subplots(figsize=(8,6))     bars = ax1.bar(base[group].astype(str), base['Number of observations'], label='Number of observations')     ax1.bar_label(bars, padding=3)     ax1.set_ylabel('Number of observations')     ax1.set_xlabel(group)     ax2 = ax1.twinx()     ax2.plot(base[group].astype(str), base['Failure percentage'], marker='o', linewidth=2, color='red', label='Failure percentage')     for i, v in enumerate(base['Failure percentage']):         ax2.text(i, v, f'{v:.1%}', ha='center', va='bottom')     ax2.set_ylabel('Failure (%)')     ax1.set_title(f\"Number of observations versus Failure percentage\\nConsidering {group}\", fontsize=26, pad=20)     ticks = ax2.get_yticks()     ax2.set_yticklabels([f'{t:.1%}' for t in ticks])     fig.subplots_adjust(left=0.15, right=0.95, top=0.85, bottom=0.15)     plt.show()  def heatmap_failure(data):     data_copy = data.copy()     matrix_data = (data_copy.groupby(['Preset_1','Preset_2'])['Fail'].mean()         .mul(100).unstack(fill_value=0).sort_index())     matrix_data.columns = matrix_data.columns.astype(str)     matrix_data.index = matrix_data.index.astype(str)     fig, ax = plt.subplots(figsize=(8,6))     im = ax.imshow(matrix_data.values, cmap='Reds', aspect='auto')     ax.set_xticks(range(matrix_data.shape[1]))     ax.set_xticklabels(matrix_data.columns)     ax.set_yticks(range(matrix_data.shape[0]))     ax.set_yticklabels(matrix_data.index)     for i in range(matrix_data.shape[0]):         for j in range(matrix_data.shape[1]):             ax.text(j, i, f\"{matrix_data.values[i, j]:.1f}\", ha='center', va='center')     ax.set_xlabel('Preset_2')     ax.set_ylabel('Preset_1')     cbar = plt.colorbar(im)     cbar.set_label('Fail %')     ax.set_title(\"Percentage Failure Heatmap\\nCombining Preset_1 and Preset_2\", fontsize=26, pad=20)     fig.subplots_adjust(left=0.15, right=0.95, top=0.85, bottom=0.15)     plt.show()  def heatmap_configuration_changes(data, group):     data_copy = data.copy()     data_copy[f'Previous {group}'] = data_copy[group].shift()     data_copy = data_copy.dropna()     data_copy[f'Previous {group}'] = data_copy[f'Previous {group}'].astype(int)     data_copy[f'Previous {group}'] = data_copy[f'Previous {group}'].astype(str)     data_copy[group] = data_copy[group].astype(str)     n_changes_df = (         data_copy         [[f'Previous {group}', group]]         .value_counts()         .reset_index(name=\"Number of Changes\")     )     n_changes_df[\"Percent of changes during operation\"] = n_changes_df[\"Number of Changes\"] / data_copy.shape[0]     n_changes_df = (         n_changes_df         .rename(columns={             f'Previous {group}':\"From\",             group:\"To\"             }         )         .sort_values(by=[\"From\", \"To\"])         .reset_index(drop=True)     )     order_x = sorted(n_changes_df[\"From\"].unique())     order_y = sorted(n_changes_df[\"To\"].unique())     fig, ax = plt.subplots(figsize=(8,6))     z = np.zeros((len(order_y), len(order_x)))     for i, y in enumerate(order_y):         for j, x in enumerate(order_x):             v = n_changes_df.loc[(n_changes_df[\"From\"]==x) &amp; (n_changes_df[\"To\"]==y), \"Percent of changes during operation\"].values             z[i, j] = v[0] if len(v) else 0     vmax = z.max() if z.max() &gt; 0 else 1     vmin = z[z&gt;0].min() if np.any(z&gt;0) else 0     im = ax.imshow(z, cmap=\"Reds\", aspect=\"auto\", vmin=vmin, vmax=vmax)     ax.set_xticks(range(len(order_x)))     ax.set_xticklabels(order_x)     ax.set_yticks(range(len(order_y)))     ax.set_yticklabels(order_y)     for i in range(len(order_y)):         for j in range(len(order_x)):             ax.text(j, i, f\"{z[i,j]:.2%}\", ha=\"center\", va=\"center\")     ticks = np.linspace(vmin, vmax, 6)     cbar = plt.colorbar(im, ticks=ticks)     cbar.set_ticklabels([f\"{t:.0%}\" for t in ticks])     cbar.set_label(\"Percent of changes during operation\")     ax.set_title(f\"Percent of changes from {group} configurations during operation\", fontsize=26, pad=20)     fig.subplots_adjust(left=0.15, right=0.95, top=0.85, bottom=0.15)     if n_changes_df.shape[0] &gt; 200:         fig.set_figheight(9)         fig.set_figwidth(20)     plt.show()     def boxplots(data, group, variable):     data_copy = data.copy()     model = smf.ols(f'{variable} ~ C({group})', data=data_copy).fit()     resid = model.resid     fitted = model.fittedvalues     fig = st.probplot(resid, dist=\"norm\", plot=plt)     plt.title(f\"QQ plot of residuals - {variable} ~ C({group})\")     plt.show()     plt.scatter(fitted, resid, s=8, alpha=0.6)     plt.axhline(0, lw=1, color='k')     plt.xlabel(\"Fitted values\")     plt.ylabel(\"Residuals\")     plt.title(f\"Residuals vs Fitted - {variable} ~ C({group})\")     plt.show()     fig, ax = plt.subplots()     data_copy.boxplot(column=variable, by=group, ax=ax)     ax.set_xlabel(group)     ax.set_ylabel(variable)     plt.suptitle(\"\")     plt.xticks(rotation=45)     fig.subplots_adjust(top=0.85, bottom=0.2)     model = smf.ols(f'{variable} ~ C({group})', data=data_copy).fit()     anova_tbl = sm.stats.anova_lm(model, typ=2)     df1 = float(anova_tbl.loc[f'C({group})', 'df'])     df2 = float(anova_tbl.loc['Residual', 'df'])     Fval = float(anova_tbl.loc[f'C({group})', 'F'])     pval = float(anova_tbl.loc[f'C({group})', 'PR(&gt;F)'])     ss_effect = float(anova_tbl.loc[f'C({group})', 'sum_sq'])     ss_error  = float(anova_tbl.loc['Residual', 'sum_sq'])     mse = ss_error / df2     y = data_copy[variable].to_numpy(dtype=float)     ss_total = float(((y - y.mean())**2).sum())     eta2   = ss_effect / ss_total     omega2 = (ss_effect - df1 * mse) / (ss_total + mse)     k = data_copy[group].nunique()     N = len(data_copy)     text_ = (         f\"ANOVA (one-way) \u2014 {variable} ~ {group}\\n\"         f\"F({int(df1)}, {int(df2)}) = {Fval:.2f}, p = {pval:.3g} \"         f\"| \u03b7\u00b2 = {eta2:.3f}, \u03c9\u00b2 = {omega2:.3f} \"         f\"| k = {k}, N = {N}\"     )     fig.text(0.0, 0.98, text_, ha='left', va='top', bbox=dict(facecolor='white', alpha=0.75, edgecolor='black', linewidth=1))     plt.show()  In\u00a0[9]: Copied! <pre># def failure_report(data, group):\n#     data_copy = data.copy()\n\n#     total = (data_copy[group].astype(str)\n#             .value_counts().rename_axis(group)\n#             .reset_index(name='Number of observations'))\n\n#     taxa = (\n#         data_copy[data_copy[\"Fail\"]][group].astype(str).value_counts() \n#         / data_copy[group].astype(str).value_counts()\n#         ).reset_index(name='Failure percentage')\n\n#     base = (total.merge(taxa, on=group, how='left')\n#                 .fillna({'Failure percentage': 0})\n#                 .sort_values(group))\n\n#     fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n#     fig.add_trace(\n#         go.Bar(x=base[group], y=base['Number of observations'],\n#             name='Number of observations',\n#             text=base['Number of observations'], textposition='outside'),\n#         secondary_y=False\n#     )\n\n#     fig.add_trace(\n#         go.Scatter(x=base[group], y=base['Failure percentage'],\n#                 name='Failure percentage',\n#                 mode='lines+markers+text',\n#                 line=dict(color='red', width=2),\n#                 text=base['Failure percentage'],\n#                 texttemplate='%{y:.1%}',\n#                 textposition='top center',\n#                 cliponaxis=False),\n#         secondary_y=True\n#     )\n\n#     fig.update_yaxes(title_text='Number of observations', secondary_y=False)\n#     fig.update_yaxes(title_text='Failure (%)', tickformat='.1%', secondary_y=True)\n#     fig.update_xaxes(title_text=group)\n#     fig.update_layout(\n#         title=dict(\n#             text=f\"Number of observations versus Failure percentage&lt;br&gt;&lt;span style='font-size:16px;'&gt;Considering {group}&lt;/span&gt;\",\n#             x=0.5, xanchor='center',\n#             y=0.94, yanchor='top',\n#             font=dict(size=26),\n#             pad=dict(t=10, b=10)\n#         ),\n#         margin=dict(l=60, r=40, t=120, b=50)\n#     )\n#     fig.show()\n\n# def heatmap_failure(data):\n\n#     data_copy = data.copy()\n\n#     matrix_data = (data_copy.groupby(['Preset_1','Preset_2'])['Fail'].mean()\n#         .mul(100).unstack(fill_value=0).sort_index())\n\n#     matrix_data.columns = matrix_data.columns.astype(str)\n#     matrix_data.index = matrix_data.index.astype(str)\n\n#     fig = px.imshow(matrix_data, text_auto='.1f', aspect='auto',\n#             color_continuous_scale='Reds',\n#             labels=dict(color='Fail %')).update_layout(\n#         xaxis_title='Preset_2', yaxis_title='Preset_1'\n#     )\n\n#     fig.update_layout(\n#         title=dict(\n#             text=\"Percentage Failure Heatmap&lt;br&gt;&lt;span style='font-size:16px;'&gt;Combining Preset_1 and Preset_2&lt;/span&gt;\",\n#             x=0.5, xanchor='center',\n#             y=0.94, yanchor='top',\n#             font=dict(size=26),\n#             pad=dict(t=10, b=10)\n#         ),\n#         margin=dict(l=60, r=40, t=120, b=50)\n#     )\n    \n    \n#     fig.show()\n\n# def heatmap_configuration_changes(data, group):\n\n#     data_copy = data.copy()\n\n#     data_copy[f'Previous {group}'] = data_copy[group].shift()\n\n#     data_copy = data_copy.dropna()\n\n#     data_copy[f'Previous {group}'] = data_copy[f'Previous {group}'].astype(int)\n\n#     data_copy[f'Previous {group}'] = data_copy[f'Previous {group}'].astype(str)\n\n#     data_copy[group] = data_copy[group].astype(str)\n\n#     n_changes_df = (\n#         data_copy\n#         [[f'Previous {group}', group]]\n#         .value_counts()\n#         .reset_index(name=\"Number of Changes\")\n#     )\n\n#     n_changes_df[\"Percent of changes during operation\"] = n_changes_df[\"Number of Changes\"] / data_copy.shape[0]\n\n#     n_changes_df = (\n#         n_changes_df\n#         .rename(columns={\n#             f'Previous {group}':\"From\",\n#             group:\"To\"\n#             }\n#         )\n#         .sort_values(by=[\"From\", \"To\"])\n#         .reset_index(drop=True)\n#     )\n\n#     order_x = sorted(n_changes_df[\"From\"].unique())\n#     order_y = sorted(n_changes_df[\"To\"].unique())\n\n#     fig = px.density_heatmap(\n#         n_changes_df,\n#         x=\"From\", y=\"To\", z=\"Percent of changes during operation\",\n#         histfunc=\"sum\",\n#         text_auto=True,\n#         category_orders={\"From\": order_x, \"To\": order_y},\n#         labels={\"From\": \"From\", \"To\": \"To\", \"Percent of changes during operation\": \"Percent of changes during operation\"},\n#         color_continuous_scale=\"Reds\"\n#     )\n\n#     fig.update_traces(\n#         meta = np.array([[(n_changes_df.loc[(n_changes_df[\"From\"]==x) &amp; (n_changes_df[\"To\"]==y),\n#                                        \"Number of Changes\"].values[0]) if (n_changes_df.loc[(n_changes_df[\"From\"]==x) &amp; (n_changes_df[\"To\"]==y),\n#                                        \"Number of Changes\"].values) else 0\n#                       for x in order_x] for y in order_y[::-1]]),\n#         hovertemplate = \"From=%{x}&lt;br&gt;To=%{y}&lt;br&gt;Percent=%{z:.2%}&lt;br&gt;Number of Changes=%{meta}&lt;extra&gt;&lt;/extra&gt;\",\n#         texttemplate=\"%{z:.2%}\"\n#     )\n#     fig.update_layout(\n#         coloraxis_colorbar=dict(\n#             title=\"Percent of changes during operation\",\n#             tickformat=\".2%\"\n#         )\n#     )\n\n#     fig.update_layout(\n#             title=dict(\n#                 text=f\"Percent of changes from {group} configurations during operation\",\n#                 x=0.5, xanchor='center',\n#                 y=0.94, yanchor='top',\n#                 font=dict(size=26),\n#                 pad=dict(t=10, b=10)\n#             ),\n#             margin=dict(l=60, r=40, t=120, b=50)\n#         )\n#     if n_changes_df.shape[0] &gt; 20:\n#         fig.update_layout(height=900)\n\n#     fig.show()\n\n# def boxplots(data, group, variable):\n\n#     data_copy = data.copy()\n\n#     model = smf.ols(f'{variable} ~ C({group})', data=data_copy).fit()\n#     resid = model.resid\n#     fitted = model.fittedvalues\n\n\n#     fig = st.probplot(resid, dist=\"norm\", plot=plt)\n#     plt.title(f\"QQ plot of residuals - {variable} ~ C({group})\")\n#     plt.show()\n\n\n#     plt.scatter(fitted, resid, s=8, alpha=0.6)\n#     plt.axhline(0, lw=1, color='k')\n#     plt.xlabel(\"Fitted values\")\n#     plt.ylabel(\"Residuals\")\n#     plt.title(f\"Residuals vs Fitted - {variable} ~ C({group})\")\n#     plt.show()\n\n#     fig = px.box(data_copy, x=group, y=variable)\n#     fig.update_layout(\n#         xaxis_title=group, yaxis_title=variable,\n#         xaxis_tickangle=45, margin=dict(t=120, b=100)\n#     )\n\n#     model = smf.ols(f'{variable} ~ C({group})', data=data_copy).fit()\n#     anova_tbl = sm.stats.anova_lm(model, typ=2)\n\n#     df1 = float(anova_tbl.loc[f'C({group})', 'df'])\n#     df2 = float(anova_tbl.loc['Residual', 'df'])\n#     Fval = float(anova_tbl.loc[f'C({group})', 'F'])\n#     pval = float(anova_tbl.loc[f'C({group})', 'PR(&gt;F)'])\n#     ss_effect = float(anova_tbl.loc[f'C({group})', 'sum_sq'])\n#     ss_error  = float(anova_tbl.loc['Residual', 'sum_sq'])\n#     mse = ss_error / df2\n\n#     y = data_copy[variable].to_numpy(dtype=float)\n#     ss_total = float(((y - y.mean())**2).sum())\n\n#     eta2   = ss_effect / ss_total\n#     omega2 = (ss_effect - df1 * mse) / (ss_total + mse)\n\n#     k = data_copy[group].nunique()\n#     N = len(data_copy)\n\n\n#     text_ = (\n#         f\"ANOVA (one-way) \u2014 {variable} ~ {group}&lt;br&gt;\"\n#         f\"F({int(df1)}, {int(df2)}) = {Fval:.2f}, p = {pval:.3g} \"\n#         f\"| \u03b7\u00b2 = {eta2:.3f}, \u03c9\u00b2 = {omega2:.3f} \"\n#         f\"| k = {k}, N = {N}\"\n#     )\n\n#     fig.add_annotation(\n#         xref='paper', yref='paper', x=0.0, y=1.12,\n#         text=text_, showarrow=False, align='left',\n#         bgcolor='rgba(255,255,255,0.75)', bordercolor='black', borderwidth=1,\n#         font=dict(size=12)\n#     )\n\n#     fig.show()\n</pre> # def failure_report(data, group): #     data_copy = data.copy()  #     total = (data_copy[group].astype(str) #             .value_counts().rename_axis(group) #             .reset_index(name='Number of observations'))  #     taxa = ( #         data_copy[data_copy[\"Fail\"]][group].astype(str).value_counts()  #         / data_copy[group].astype(str).value_counts() #         ).reset_index(name='Failure percentage')  #     base = (total.merge(taxa, on=group, how='left') #                 .fillna({'Failure percentage': 0}) #                 .sort_values(group))  #     fig = make_subplots(specs=[[{\"secondary_y\": True}]])  #     fig.add_trace( #         go.Bar(x=base[group], y=base['Number of observations'], #             name='Number of observations', #             text=base['Number of observations'], textposition='outside'), #         secondary_y=False #     )  #     fig.add_trace( #         go.Scatter(x=base[group], y=base['Failure percentage'], #                 name='Failure percentage', #                 mode='lines+markers+text', #                 line=dict(color='red', width=2), #                 text=base['Failure percentage'], #                 texttemplate='%{y:.1%}', #                 textposition='top center', #                 cliponaxis=False), #         secondary_y=True #     )  #     fig.update_yaxes(title_text='Number of observations', secondary_y=False) #     fig.update_yaxes(title_text='Failure (%)', tickformat='.1%', secondary_y=True) #     fig.update_xaxes(title_text=group) #     fig.update_layout( #         title=dict( #             text=f\"Number of observations versus Failure percentageConsidering {group}\", #             x=0.5, xanchor='center', #             y=0.94, yanchor='top', #             font=dict(size=26), #             pad=dict(t=10, b=10) #         ), #         margin=dict(l=60, r=40, t=120, b=50) #     ) #     fig.show()  # def heatmap_failure(data):  #     data_copy = data.copy()  #     matrix_data = (data_copy.groupby(['Preset_1','Preset_2'])['Fail'].mean() #         .mul(100).unstack(fill_value=0).sort_index())  #     matrix_data.columns = matrix_data.columns.astype(str) #     matrix_data.index = matrix_data.index.astype(str)  #     fig = px.imshow(matrix_data, text_auto='.1f', aspect='auto', #             color_continuous_scale='Reds', #             labels=dict(color='Fail %')).update_layout( #         xaxis_title='Preset_2', yaxis_title='Preset_1' #     )  #     fig.update_layout( #         title=dict( #             text=\"Percentage Failure HeatmapCombining Preset_1 and Preset_2\", #             x=0.5, xanchor='center', #             y=0.94, yanchor='top', #             font=dict(size=26), #             pad=dict(t=10, b=10) #         ), #         margin=dict(l=60, r=40, t=120, b=50) #     )           #     fig.show()  # def heatmap_configuration_changes(data, group):  #     data_copy = data.copy()  #     data_copy[f'Previous {group}'] = data_copy[group].shift()  #     data_copy = data_copy.dropna()  #     data_copy[f'Previous {group}'] = data_copy[f'Previous {group}'].astype(int)  #     data_copy[f'Previous {group}'] = data_copy[f'Previous {group}'].astype(str)  #     data_copy[group] = data_copy[group].astype(str)  #     n_changes_df = ( #         data_copy #         [[f'Previous {group}', group]] #         .value_counts() #         .reset_index(name=\"Number of Changes\") #     )  #     n_changes_df[\"Percent of changes during operation\"] = n_changes_df[\"Number of Changes\"] / data_copy.shape[0]  #     n_changes_df = ( #         n_changes_df #         .rename(columns={ #             f'Previous {group}':\"From\", #             group:\"To\" #             } #         ) #         .sort_values(by=[\"From\", \"To\"]) #         .reset_index(drop=True) #     )  #     order_x = sorted(n_changes_df[\"From\"].unique()) #     order_y = sorted(n_changes_df[\"To\"].unique())  #     fig = px.density_heatmap( #         n_changes_df, #         x=\"From\", y=\"To\", z=\"Percent of changes during operation\", #         histfunc=\"sum\", #         text_auto=True, #         category_orders={\"From\": order_x, \"To\": order_y}, #         labels={\"From\": \"From\", \"To\": \"To\", \"Percent of changes during operation\": \"Percent of changes during operation\"}, #         color_continuous_scale=\"Reds\" #     )  #     fig.update_traces( #         meta = np.array([[(n_changes_df.loc[(n_changes_df[\"From\"]==x) &amp; (n_changes_df[\"To\"]==y), #                                        \"Number of Changes\"].values[0]) if (n_changes_df.loc[(n_changes_df[\"From\"]==x) &amp; (n_changes_df[\"To\"]==y), #                                        \"Number of Changes\"].values) else 0 #                       for x in order_x] for y in order_y[::-1]]), #         hovertemplate = \"From=%{x}To=%{y}Percent=%{z:.2%}Number of Changes=%{meta}\", #         texttemplate=\"%{z:.2%}\" #     ) #     fig.update_layout( #         coloraxis_colorbar=dict( #             title=\"Percent of changes during operation\", #             tickformat=\".2%\" #         ) #     )  #     fig.update_layout( #             title=dict( #                 text=f\"Percent of changes from {group} configurations during operation\", #                 x=0.5, xanchor='center', #                 y=0.94, yanchor='top', #                 font=dict(size=26), #                 pad=dict(t=10, b=10) #             ), #             margin=dict(l=60, r=40, t=120, b=50) #         ) #     if n_changes_df.shape[0] &gt; 20: #         fig.update_layout(height=900)  #     fig.show()  # def boxplots(data, group, variable):  #     data_copy = data.copy()  #     model = smf.ols(f'{variable} ~ C({group})', data=data_copy).fit() #     resid = model.resid #     fitted = model.fittedvalues   #     fig = st.probplot(resid, dist=\"norm\", plot=plt) #     plt.title(f\"QQ plot of residuals - {variable} ~ C({group})\") #     plt.show()   #     plt.scatter(fitted, resid, s=8, alpha=0.6) #     plt.axhline(0, lw=1, color='k') #     plt.xlabel(\"Fitted values\") #     plt.ylabel(\"Residuals\") #     plt.title(f\"Residuals vs Fitted - {variable} ~ C({group})\") #     plt.show()  #     fig = px.box(data_copy, x=group, y=variable) #     fig.update_layout( #         xaxis_title=group, yaxis_title=variable, #         xaxis_tickangle=45, margin=dict(t=120, b=100) #     )  #     model = smf.ols(f'{variable} ~ C({group})', data=data_copy).fit() #     anova_tbl = sm.stats.anova_lm(model, typ=2)  #     df1 = float(anova_tbl.loc[f'C({group})', 'df']) #     df2 = float(anova_tbl.loc['Residual', 'df']) #     Fval = float(anova_tbl.loc[f'C({group})', 'F']) #     pval = float(anova_tbl.loc[f'C({group})', 'PR(&gt;F)']) #     ss_effect = float(anova_tbl.loc[f'C({group})', 'sum_sq']) #     ss_error  = float(anova_tbl.loc['Residual', 'sum_sq']) #     mse = ss_error / df2  #     y = data_copy[variable].to_numpy(dtype=float) #     ss_total = float(((y - y.mean())**2).sum())  #     eta2   = ss_effect / ss_total #     omega2 = (ss_effect - df1 * mse) / (ss_total + mse)  #     k = data_copy[group].nunique() #     N = len(data_copy)   #     text_ = ( #         f\"ANOVA (one-way) \u2014 {variable} ~ {group}\" #         f\"F({int(df1)}, {int(df2)}) = {Fval:.2f}, p = {pval:.3g} \" #         f\"| \u03b7\u00b2 = {eta2:.3f}, \u03c9\u00b2 = {omega2:.3f} \" #         f\"| k = {k}, N = {N}\" #     )  #     fig.add_annotation( #         xref='paper', yref='paper', x=0.0, y=1.12, #         text=text_, showarrow=False, align='left', #         bgcolor='rgba(255,255,255,0.75)', bordercolor='black', borderwidth=1, #         font=dict(size=12) #     )  #     fig.show() In\u00a0[38]: Copied! <pre>failure_report(df, \"Preset_1\")\nfailure_report(df, \"Preset_2\")\n\naux_data = df.copy()\naux_data[\"Preset_1_2\"] = df['Preset_1'].astype(str) + df['Preset_2'].astype(str)\n\nfailure_report(aux_data, \"Preset_1_2\")\n</pre> failure_report(df, \"Preset_1\") failure_report(df, \"Preset_2\")  aux_data = df.copy() aux_data[\"Preset_1_2\"] = df['Preset_1'].astype(str) + df['Preset_2'].astype(str)  failure_report(aux_data, \"Preset_1_2\") <pre>/tmp/ipykernel_610/2274624694.py:25: UserWarning:\n\nset_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n\n</pre> <pre>/tmp/ipykernel_610/2274624694.py:25: UserWarning:\n\nset_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n\n</pre> <pre>/tmp/ipykernel_610/2274624694.py:25: UserWarning:\n\nset_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n\n</pre> In\u00a0[39]: Copied! <pre>heatmap_failure(df)\n</pre> heatmap_failure(df) In\u00a0[42]: Copied! <pre>heatmap_configuration_changes(df, \"Preset_1\")\nheatmap_configuration_changes(df, \"Preset_2\")\n\naux_data = df.copy()\naux_data[\"Preset_1_2\"] = df['Preset_1'].astype(str) + df['Preset_2'].astype(str)\n\nheatmap_configuration_changes(aux_data, \"Preset_1_2\")\n</pre> heatmap_configuration_changes(df, \"Preset_1\") heatmap_configuration_changes(df, \"Preset_2\")  aux_data = df.copy() aux_data[\"Preset_1_2\"] = df['Preset_1'].astype(str) + df['Preset_2'].astype(str)  heatmap_configuration_changes(aux_data, \"Preset_1_2\") In\u00a0[43]: Copied! <pre>for column in df.columns[3:-1]:\n    boxplots(df, 'Preset_1', column)\n\nfor column in df.columns[3:-1]:\n    boxplots(df, 'Preset_2', column)\n\naux = df.copy()\naux['Preset_1_2'] = aux['Preset_1'].astype(str) + '-' + aux['Preset_2'].astype(str)\nfor column in df.columns[3:-1]:\n    boxplots(aux, 'Preset_1_2', column)\n</pre> for column in df.columns[3:-1]:     boxplots(df, 'Preset_1', column)  for column in df.columns[3:-1]:     boxplots(df, 'Preset_2', column)  aux = df.copy() aux['Preset_1_2'] = aux['Preset_1'].astype(str) + '-' + aux['Preset_2'].astype(str) for column in df.columns[3:-1]:     boxplots(aux, 'Preset_1_2', column) In\u00a0[44]: Copied! <pre>def cycles_per_variable(data, x_axis, variable):\n    data_copy = data.copy()\n    data_copy[variable].quantile(0.95)\n    data_copy[variable].quantile(0.05)\n    p05 = data_copy[variable].quantile(0.05)\n    p95 = data_copy[variable].quantile(0.95)\n    median = data_copy[variable].median()\n    df_sorted = data_copy.sort_values([x_axis])\n    fig, ax = plt.subplots(figsize=(8,6))\n    ax.plot(df_sorted[x_axis], df_sorted[variable], color=\"red\", linewidth=2, label=variable)\n    ax.fill_between(df_sorted[x_axis], p05, p95, alpha=0.2, label='P05\u2013P95')\n    ax.axhline(median, linestyle='--', linewidth=2, color='blue', label='Median')\n    ax.axhline(p95, linestyle=':', linewidth=1)\n    ax.axhline(p05, linestyle=':', linewidth=1)\n    for x in data_copy.loc[data_copy[\"Fail\"], x_axis].unique():\n        ax.axvline(x=x, linewidth=2, color=\"black\", alpha=0.5)\n    ax.plot([], [], color=\"black\", alpha=0.5, linewidth=1, label=\"Failure events\")\n    ax.set_ylim(0, 1.1 * data_copy[variable].max())\n    ax.set_title(f\"{variable} per {x_axis} with Failure events\", fontsize=26, pad=20)\n    ax.set_xlabel(x_axis)\n    ax.set_ylabel(variable)\n    ax.legend()\n    fig.subplots_adjust(left=0.15, right=0.95, top=0.85, bottom=0.15)\n    plt.show()\n\ndef failure_3d_plot(data, x_value, y_value, z_value):\n    data_copy = data.copy()\n    fig = plt.figure(figsize=(8,6))\n    ax = fig.add_subplot(111, projection='3d')\n    d_true = data_copy[data_copy[\"Fail\"]==True]\n    d_false = data_copy[data_copy[\"Fail\"]==False]\n    ax.scatter(d_true[x_value], d_true[y_value], d_true[z_value], c=\"black\", s=3, label=\"True\")\n    ax.scatter(d_false[x_value], d_false[y_value], d_false[z_value], c=\"lightgray\", s=3, label=\"False\")\n    ax.set_title(f\"3d Plot {x_value} versus {y_value} versus {z_value} per Failure event\", fontsize=26, pad=20)\n    ax.set_xlabel(x_value)\n    ax.set_ylabel(y_value)\n    ax.set_zlabel(z_value)\n    ax.legend(title=\"Fail\")\n    fig.subplots_adjust(left=0.0, right=1.0, top=0.9, bottom=0.0)\n    plt.show()\n\ndef failure_2d_plot(data, x_value, y_value):\n    data_copy = data.copy()\n    fig, ax = plt.subplots(figsize=(8,6))\n    d_true = data_copy[data_copy[\"Fail\"]==True]\n    d_false = data_copy[data_copy[\"Fail\"]==False]\n    ax.scatter(d_true[x_value], d_true[y_value], c=\"black\", s=20, label=\"True\")\n    ax.scatter(d_false[x_value], d_false[y_value], c=\"lightgray\", s=20, label=\"False\")\n    ax.set_title(f\"Scatter Plot {x_value} versus {y_value} per Failure event\", fontsize=26, pad=20)\n    ax.set_xlabel(x_value)\n    ax.set_ylabel(y_value)\n    ax.legend(title=\"Fail\")\n    fig.subplots_adjust(left=0.15, right=0.95, top=0.85, bottom=0.15)\n    plt.show()\n</pre> def cycles_per_variable(data, x_axis, variable):     data_copy = data.copy()     data_copy[variable].quantile(0.95)     data_copy[variable].quantile(0.05)     p05 = data_copy[variable].quantile(0.05)     p95 = data_copy[variable].quantile(0.95)     median = data_copy[variable].median()     df_sorted = data_copy.sort_values([x_axis])     fig, ax = plt.subplots(figsize=(8,6))     ax.plot(df_sorted[x_axis], df_sorted[variable], color=\"red\", linewidth=2, label=variable)     ax.fill_between(df_sorted[x_axis], p05, p95, alpha=0.2, label='P05\u2013P95')     ax.axhline(median, linestyle='--', linewidth=2, color='blue', label='Median')     ax.axhline(p95, linestyle=':', linewidth=1)     ax.axhline(p05, linestyle=':', linewidth=1)     for x in data_copy.loc[data_copy[\"Fail\"], x_axis].unique():         ax.axvline(x=x, linewidth=2, color=\"black\", alpha=0.5)     ax.plot([], [], color=\"black\", alpha=0.5, linewidth=1, label=\"Failure events\")     ax.set_ylim(0, 1.1 * data_copy[variable].max())     ax.set_title(f\"{variable} per {x_axis} with Failure events\", fontsize=26, pad=20)     ax.set_xlabel(x_axis)     ax.set_ylabel(variable)     ax.legend()     fig.subplots_adjust(left=0.15, right=0.95, top=0.85, bottom=0.15)     plt.show()  def failure_3d_plot(data, x_value, y_value, z_value):     data_copy = data.copy()     fig = plt.figure(figsize=(8,6))     ax = fig.add_subplot(111, projection='3d')     d_true = data_copy[data_copy[\"Fail\"]==True]     d_false = data_copy[data_copy[\"Fail\"]==False]     ax.scatter(d_true[x_value], d_true[y_value], d_true[z_value], c=\"black\", s=3, label=\"True\")     ax.scatter(d_false[x_value], d_false[y_value], d_false[z_value], c=\"lightgray\", s=3, label=\"False\")     ax.set_title(f\"3d Plot {x_value} versus {y_value} versus {z_value} per Failure event\", fontsize=26, pad=20)     ax.set_xlabel(x_value)     ax.set_ylabel(y_value)     ax.set_zlabel(z_value)     ax.legend(title=\"Fail\")     fig.subplots_adjust(left=0.0, right=1.0, top=0.9, bottom=0.0)     plt.show()  def failure_2d_plot(data, x_value, y_value):     data_copy = data.copy()     fig, ax = plt.subplots(figsize=(8,6))     d_true = data_copy[data_copy[\"Fail\"]==True]     d_false = data_copy[data_copy[\"Fail\"]==False]     ax.scatter(d_true[x_value], d_true[y_value], c=\"black\", s=20, label=\"True\")     ax.scatter(d_false[x_value], d_false[y_value], c=\"lightgray\", s=20, label=\"False\")     ax.set_title(f\"Scatter Plot {x_value} versus {y_value} per Failure event\", fontsize=26, pad=20)     ax.set_xlabel(x_value)     ax.set_ylabel(y_value)     ax.legend(title=\"Fail\")     fig.subplots_adjust(left=0.15, right=0.95, top=0.85, bottom=0.15)     plt.show()  In\u00a0[45]: Copied! <pre>for column in df.columns[3:-1]:\n    cycles_per_variable(df, \"Cycle\", column)\n\naux_data = df.copy()\n\naux_data[\"Module Vibration\"] = aux_data.apply(lambda x: ((x[\"VibrationX\"]**2) + (x[\"VibrationY\"]**2) + (x[\"VibrationZ\"]**2))**(1/2), axis=1)\n\ncycles_per_variable(aux_data, \"Cycle\", \"Module Vibration\")\n\nfailure_2d_plot(df, \"Temperature\", \"Pressure\")\n\nfailure_2d_plot(aux_data, \"Module Vibration\", \"Frequency\")\n\nfailure_2d_plot(aux_data, \"Module Vibration\", \"Temperature\")\n\nfailure_2d_plot(aux_data, \"Module Vibration\", \"Pressure\")\n\nfailure_3d_plot(df, \"Temperature\", \"Pressure\", \"Frequency\")\n\nfailure_3d_plot(df, \"VibrationX\", \"VibrationY\", \"VibrationZ\")\n</pre> for column in df.columns[3:-1]:     cycles_per_variable(df, \"Cycle\", column)  aux_data = df.copy()  aux_data[\"Module Vibration\"] = aux_data.apply(lambda x: ((x[\"VibrationX\"]**2) + (x[\"VibrationY\"]**2) + (x[\"VibrationZ\"]**2))**(1/2), axis=1)  cycles_per_variable(aux_data, \"Cycle\", \"Module Vibration\")  failure_2d_plot(df, \"Temperature\", \"Pressure\")  failure_2d_plot(aux_data, \"Module Vibration\", \"Frequency\")  failure_2d_plot(aux_data, \"Module Vibration\", \"Temperature\")  failure_2d_plot(aux_data, \"Module Vibration\", \"Pressure\")  failure_3d_plot(df, \"Temperature\", \"Pressure\", \"Frequency\")  failure_3d_plot(df, \"VibrationX\", \"VibrationY\", \"VibrationZ\") In\u00a0[231]: Copied! <pre>def test_stationarity(series, scope):\n\n    print(f'Dickey-Fuller (Dataset {scope}):')\n    adf_test = adfuller(series.dropna())\n\n    adf_stat = adf_test[0]\n    p_value = adf_test[1]\n    critical_values = adf_test[4]\n\n\n    print(f'ADF statistic: {adf_stat}')\n    print(f'p-value: {p_value}')\n    print('Critical values:')\n    for key, value in critical_values.items():\n        print(f'\\t{key}: {value}')\n\n\n    if adf_stat &lt; critical_values['5%']:\n        print(\"The series is stationary (Reject the null hypothesis)\")\n    else:\n        print(\"The series is NOT stationary (Fail to reject the null hypothesis)\")\n\n\nnumerical_features = [\"Temperature\", \"Pressure\", \"VibrationX\", \"VibrationY\", \"VibrationZ\", \"Frequency\"]\nfor col in numerical_features:\n    s = df.sort_values('Cycle')[col]\n    test_stationarity(s, col)\n</pre> def test_stationarity(series, scope):      print(f'Dickey-Fuller (Dataset {scope}):')     adf_test = adfuller(series.dropna())      adf_stat = adf_test[0]     p_value = adf_test[1]     critical_values = adf_test[4]       print(f'ADF statistic: {adf_stat}')     print(f'p-value: {p_value}')     print('Critical values:')     for key, value in critical_values.items():         print(f'\\t{key}: {value}')       if adf_stat &lt; critical_values['5%']:         print(\"The series is stationary (Reject the null hypothesis)\")     else:         print(\"The series is NOT stationary (Fail to reject the null hypothesis)\")   numerical_features = [\"Temperature\", \"Pressure\", \"VibrationX\", \"VibrationY\", \"VibrationZ\", \"Frequency\"] for col in numerical_features:     s = df.sort_values('Cycle')[col]     test_stationarity(s, col) <pre>Dickey-Fuller (Dataset Temperature):\nADF statistic: -12.988680950011224\np-value: 2.8425779537946296e-24\nCritical values:\n\t1%: -3.438581476199162\n\t5%: -2.865173218890781\n\t10%: -2.56870466056054\nThe series is stationary (Reject the null hypothesis)\nDickey-Fuller (Dataset Pressure):\nADF statistic: -8.973295804703648\np-value: 7.6502921761507e-15\nCritical values:\n\t1%: -3.438602251755426\n\t5%: -2.8651823762743245\n\t10%: -2.5687095387840673\nThe series is stationary (Reject the null hypothesis)\nDickey-Fuller (Dataset VibrationX):\nADF statistic: -6.9958169063199565\np-value: 7.538878106337241e-10\nCritical values:\n\t1%: -3.438623132449471\n\t5%: -2.8651915799370014\n\t10%: -2.568714441670417\nThe series is stationary (Reject the null hypothesis)\nDickey-Fuller (Dataset VibrationY):\nADF statistic: -8.009749176554534\np-value: 2.2177469565403674e-12\nCritical values:\n\t1%: -3.438602251755426\n\t5%: -2.8651823762743245\n\t10%: -2.5687095387840673\nThe series is stationary (Reject the null hypothesis)\nDickey-Fuller (Dataset VibrationZ):\nADF statistic: -6.648510831277863\np-value: 5.191805426666831e-09\nCritical values:\n\t1%: -3.438633612472885\n\t5%: -2.865196199232788\n\t10%: -2.5687169024206713\nThe series is stationary (Reject the null hypothesis)\nDickey-Fuller (Dataset Frequency):\nADF statistic: -9.59674762354758\np-value: 1.9690415765210325e-16\nCritical values:\n\t1%: -3.438602251755426\n\t5%: -2.8651823762743245\n\t10%: -2.5687095387840673\nThe series is stationary (Reject the null hypothesis)\n</pre> In\u00a0[232]: Copied! <pre>def kde(df, numerical_columns, target_col):\n\n    num_cols = len(numerical_columns)\n    plt.figure(figsize=(18, 15))\n\n    for i, column in enumerate(numerical_columns):\n        plt.subplot(num_cols, 2, 2*i+1)\n        sns.kdeplot(data=df, x=column, hue=target_col, fill=True)\n        plt.title(f'KDE distribution of {column} by {target_col}')\n\n\n    plt.tight_layout()\n    plt.show()\n\nnumerical_features = [\"Temperature\", \"Pressure\", \"VibrationX\", \"VibrationY\", \"VibrationZ\", \"Frequency\"]\n\nkde(df, numerical_features, 'Fail')\n</pre> def kde(df, numerical_columns, target_col):      num_cols = len(numerical_columns)     plt.figure(figsize=(18, 15))      for i, column in enumerate(numerical_columns):         plt.subplot(num_cols, 2, 2*i+1)         sns.kdeplot(data=df, x=column, hue=target_col, fill=True)         plt.title(f'KDE distribution of {column} by {target_col}')       plt.tight_layout()     plt.show()  numerical_features = [\"Temperature\", \"Pressure\", \"VibrationX\", \"VibrationY\", \"VibrationZ\", \"Frequency\"]  kde(df, numerical_features, 'Fail')  In\u00a0[233]: Copied! <pre>def plot_correlation_heatmap(df,features):\n    corr_matrix = df[features].corr()\n\n    plt.figure(figsize=(8, 3))\n\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n\n    plt.title('Heatmap of numerical features correlation')\n    plt.show()\n\nplot_correlation_heatmap(df,numerical_features)\n</pre> def plot_correlation_heatmap(df,features):     corr_matrix = df[features].corr()      plt.figure(figsize=(8, 3))      sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)      plt.title('Heatmap of numerical features correlation')     plt.show()  plot_correlation_heatmap(df,numerical_features) In\u00a0[234]: Copied! <pre>def preprocess(data,drops, columns, target):\n  data_copy = data.copy()\n  y = data_copy[target]\n  X = data_copy.drop(drops, axis=1)\n\n  scaler = StandardScaler()\n  X[columns] = scaler.fit_transform(X[columns])\n\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n  return  X_train, X_test, y_train, y_test\n\ndef train_model(X_train, y_train, model_exp):\n\n    scorer = make_scorer(recall_score, pos_label=1)\n\n    if model_exp == 'Random Forest':\n        model = RandomForestClassifier(random_state=42)\n        param_grid = {\n            'n_estimators': [100, 200, 300],\n            'max_depth': [10, 20, None],\n            'min_samples_split': [2, 5, 10]\n        }\n\n    elif model_exp == 'SVM':\n        model = SVC(kernel='rbf', random_state=42)\n        param_grid = {\n            'C': [0.1, 1, 10],\n            'gamma': ['scale', 'auto'],\n            'kernel': ['rbf']\n        }\n\n    elif model_exp == 'Logistic Regression':\n        model = LogisticRegression(max_iter=1000, solver='saga', n_jobs=-1)\n        param_grid = {\n            'C': [0.01, 0.1, 1, 10],\n            'penalty': ['l1', 'l2']\n        }\n\n    elif model_exp == 'Extra Trees':\n        model = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n        param_grid = {\n            'n_estimators': [200, 400],\n            'max_depth': [None, 10, 20],\n            'min_samples_split': [2, 10]\n        }\n\n    elif model_exp == 'Gradient Boosting':\n        model = GradientBoostingClassifier(random_state=42)\n        param_grid = {\n            'n_estimators': [100, 300],\n            'learning_rate': [0.05, 0.1],\n            'max_depth': [2, 3]\n        }\n\n    elif model_exp == 'HistGB':\n        model = HistGradientBoostingClassifier(random_state=42)\n        param_grid = {\n            'max_depth': [None, 6, 12],\n            'learning_rate': [0.05, 0.1],\n            'max_leaf_nodes': [31, 63]\n        }\n\n    elif model_exp == 'KNN':\n        model = KNeighborsClassifier()\n        param_grid = {\n            'n_neighbors': [5, 15, 30],\n            'weights': ['uniform', 'distance'],\n            'p': [1, 2]\n        }\n\n    elif model_exp == 'Linear SVM':\n        model = LinearSVC(random_state=42)\n        param_grid = {\n            'C': [0.1, 1, 10]\n        }\n\n    grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n                               scoring=scorer, cv=5, n_jobs=-1, verbose=2)\n    grid_search.fit(X_train, y_train)\n\n    print(f\"Better parameters for {model_exp}: {grid_search.best_params_}\")\n    return grid_search.best_estimator_\n\n\ndef predict_and_evaluate(model, X_test, y_test):\n\n  y_pred = model.predict(X_test)\n  conf_matrix = confusion_matrix(y_test, y_pred)\n  sns.heatmap(conf_matrix, annot=True, fmt='d')\n  plt.title('Confusion Matrix')\n  plt.show()\n  print(f'Model: {model}')\n  print(classification_report(y_test, y_pred))\n\n  f1 = f1_score(y_test, y_pred)\n  print(f'F1 score: {f1 * 100:.2f}%')\n</pre> def preprocess(data,drops, columns, target):   data_copy = data.copy()   y = data_copy[target]   X = data_copy.drop(drops, axis=1)    scaler = StandardScaler()   X[columns] = scaler.fit_transform(X[columns])    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)   return  X_train, X_test, y_train, y_test  def train_model(X_train, y_train, model_exp):      scorer = make_scorer(recall_score, pos_label=1)      if model_exp == 'Random Forest':         model = RandomForestClassifier(random_state=42)         param_grid = {             'n_estimators': [100, 200, 300],             'max_depth': [10, 20, None],             'min_samples_split': [2, 5, 10]         }      elif model_exp == 'SVM':         model = SVC(kernel='rbf', random_state=42)         param_grid = {             'C': [0.1, 1, 10],             'gamma': ['scale', 'auto'],             'kernel': ['rbf']         }      elif model_exp == 'Logistic Regression':         model = LogisticRegression(max_iter=1000, solver='saga', n_jobs=-1)         param_grid = {             'C': [0.01, 0.1, 1, 10],             'penalty': ['l1', 'l2']         }      elif model_exp == 'Extra Trees':         model = ExtraTreesClassifier(random_state=42, n_jobs=-1)         param_grid = {             'n_estimators': [200, 400],             'max_depth': [None, 10, 20],             'min_samples_split': [2, 10]         }      elif model_exp == 'Gradient Boosting':         model = GradientBoostingClassifier(random_state=42)         param_grid = {             'n_estimators': [100, 300],             'learning_rate': [0.05, 0.1],             'max_depth': [2, 3]         }      elif model_exp == 'HistGB':         model = HistGradientBoostingClassifier(random_state=42)         param_grid = {             'max_depth': [None, 6, 12],             'learning_rate': [0.05, 0.1],             'max_leaf_nodes': [31, 63]         }      elif model_exp == 'KNN':         model = KNeighborsClassifier()         param_grid = {             'n_neighbors': [5, 15, 30],             'weights': ['uniform', 'distance'],             'p': [1, 2]         }      elif model_exp == 'Linear SVM':         model = LinearSVC(random_state=42)         param_grid = {             'C': [0.1, 1, 10]         }      grid_search = GridSearchCV(estimator=model, param_grid=param_grid,                                scoring=scorer, cv=5, n_jobs=-1, verbose=2)     grid_search.fit(X_train, y_train)      print(f\"Better parameters for {model_exp}: {grid_search.best_params_}\")     return grid_search.best_estimator_   def predict_and_evaluate(model, X_test, y_test):    y_pred = model.predict(X_test)   conf_matrix = confusion_matrix(y_test, y_pred)   sns.heatmap(conf_matrix, annot=True, fmt='d')   plt.title('Confusion Matrix')   plt.show()   print(f'Model: {model}')   print(classification_report(y_test, y_pred))    f1 = f1_score(y_test, y_pred)   print(f'F1 score: {f1 * 100:.2f}%') In\u00a0[235]: Copied! <pre>df[\"Preset_1\"] = df[\"Preset_1\"].astype(str)\ndf[\"Preset_2\"] = df[\"Preset_2\"].astype(str)\ndf_hot = pd.get_dummies(df, columns=[\"Preset_1\", \"Preset_2\"], drop_first=False)\n</pre> df[\"Preset_1\"] = df[\"Preset_1\"].astype(str) df[\"Preset_2\"] = df[\"Preset_2\"].astype(str) df_hot = pd.get_dummies(df, columns=[\"Preset_1\", \"Preset_2\"], drop_first=False) In\u00a0[236]: Copied! <pre>models = ['Random Forest', 'SVM', 'Logistic Regression', 'Extra Trees', 'Gradient Boosting', 'HistGB', 'KNN', 'Linear SVM']\naux = df.copy()\naux[\"Fail\"] = aux[\"Fail\"].apply(lambda x: 1 if x else 0)\ndrops = [\"Preset_1\",  \"Preset_2\", \"Fail\", \"Cycle\"]\nX_train, X_test, y_train, y_test = preprocess(aux, drops, numerical_features, \"Fail\")\nros = RandomOverSampler(sampling_strategy=0.5, random_state=42)\nX_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\nX_train_hot, X_test_hot, y_train_hot, y_test_hot = preprocess(df_hot, [\"Fail\", \"Cycle\"], numerical_features, \"Fail\")\nX_train_hot_ros, y_train_hot_ros = ros.fit_resample(X_train_hot, y_train_hot)\n\nfor model in models:\n  print('-------------------------------------------------------------------------------')\n  print('-------------------------- Original dataset -----------------------------------')\n  print(f'Model: {model}')\n  print('-------------------------------------------------------------------------------')\n  tmodel = train_model(X_train, y_train, model)\n  predict_and_evaluate(tmodel, X_test, y_test)\n  print('-------------------------------------------------------------------------------')\n  print('-------------------------- Random oversampled ---------------------------------')\n  print(f'Model: {model}')\n  print('-------------------------------------------------------------------------------')\n  tmodel = train_model(X_train_ros, y_train_ros, model)\n  predict_and_evaluate(tmodel, X_test, y_test)\n  print('-------------------------------------------------------------------------------')\n  print('-------------------------- Original dataset one hot encoded -------------------')\n  print(f'Model: {model}')\n  print('-------------------------------------------------------------------------------')\n  tmodel = train_model(X_train_hot, y_train_hot, model)\n  predict_and_evaluate(tmodel, X_test_hot, y_test_hot)\n  print('-------------------------------------------------------------------------------')\n  print('-------------------------- Random oversampled one hot encoded -----------------')\n  print(f'Model: {model}')\n  print('-------------------------------------------------------------------------------')\n  tmodel = train_model(X_train_hot_ros, y_train_hot_ros, model)\n  predict_and_evaluate(tmodel, X_test_hot, y_test_hot)\n  print('-------------------------------------------------------------------------------')\n</pre> models = ['Random Forest', 'SVM', 'Logistic Regression', 'Extra Trees', 'Gradient Boosting', 'HistGB', 'KNN', 'Linear SVM'] aux = df.copy() aux[\"Fail\"] = aux[\"Fail\"].apply(lambda x: 1 if x else 0) drops = [\"Preset_1\",  \"Preset_2\", \"Fail\", \"Cycle\"] X_train, X_test, y_train, y_test = preprocess(aux, drops, numerical_features, \"Fail\") ros = RandomOverSampler(sampling_strategy=0.5, random_state=42) X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train) X_train_hot, X_test_hot, y_train_hot, y_test_hot = preprocess(df_hot, [\"Fail\", \"Cycle\"], numerical_features, \"Fail\") X_train_hot_ros, y_train_hot_ros = ros.fit_resample(X_train_hot, y_train_hot)  for model in models:   print('-------------------------------------------------------------------------------')   print('-------------------------- Original dataset -----------------------------------')   print(f'Model: {model}')   print('-------------------------------------------------------------------------------')   tmodel = train_model(X_train, y_train, model)   predict_and_evaluate(tmodel, X_test, y_test)   print('-------------------------------------------------------------------------------')   print('-------------------------- Random oversampled ---------------------------------')   print(f'Model: {model}')   print('-------------------------------------------------------------------------------')   tmodel = train_model(X_train_ros, y_train_ros, model)   predict_and_evaluate(tmodel, X_test, y_test)   print('-------------------------------------------------------------------------------')   print('-------------------------- Original dataset one hot encoded -------------------')   print(f'Model: {model}')   print('-------------------------------------------------------------------------------')   tmodel = train_model(X_train_hot, y_train_hot, model)   predict_and_evaluate(tmodel, X_test_hot, y_test_hot)   print('-------------------------------------------------------------------------------')   print('-------------------------- Random oversampled one hot encoded -----------------')   print(f'Model: {model}')   print('-------------------------------------------------------------------------------')   tmodel = train_model(X_train_hot_ros, y_train_hot_ros, model)   predict_and_evaluate(tmodel, X_test_hot, y_test_hot)   print('-------------------------------------------------------------------------------') <pre>-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: Random Forest\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 27 candidates, totalling 135 fits\n</pre> <pre>[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\nBetter parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n</pre> <pre>Model: RandomForestClassifier(max_depth=10, n_estimators=200, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.97      0.99      0.98       225\n           1       0.70      0.47      0.56        15\n\n    accuracy                           0.95       240\n   macro avg       0.83      0.73      0.77       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 56.00%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: Random Forest\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 27 candidates, totalling 135 fits\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\nBetter parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n</pre> <pre>Model: RandomForestClassifier(max_depth=10, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97       225\n           1       0.59      0.67      0.62        15\n\n    accuracy                           0.95       240\n   macro avg       0.78      0.82      0.80       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 62.50%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: Random Forest\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 27 candidates, totalling 135 fits\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.4s\n\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.3s\nBetter parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n</pre> <pre>Model: RandomForestClassifier(max_depth=10, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.97      0.99      0.98       225\n        True       0.78      0.47      0.58        15\n\n    accuracy                           0.96       240\n   macro avg       0.87      0.73      0.78       240\nweighted avg       0.95      0.96      0.95       240\n\nF1 score: 58.33%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: Random Forest\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 27 candidates, totalling 135 fits\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.3s\nBetter parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n</pre> <pre>Model: RandomForestClassifier(max_depth=10, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.97      0.98      0.97       225\n        True       0.62      0.53      0.57        15\n\n    accuracy                           0.95       240\n   macro avg       0.79      0.76      0.77       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 57.14%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\nBetter parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n</pre> <pre>Model: SVC(C=10, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98       225\n           1       0.63      0.80      0.71        15\n\n    accuracy                           0.96       240\n   macro avg       0.81      0.88      0.84       240\nweighted avg       0.96      0.96      0.96       240\n\nF1 score: 70.59%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\nBetter parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n</pre> <pre>Model: SVC(C=10, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97       225\n           1       0.54      0.87      0.67        15\n\n    accuracy                           0.95       240\n   macro avg       0.77      0.91      0.82       240\nweighted avg       0.96      0.95      0.95       240\n\nF1 score: 66.67%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\nBetter parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n</pre> <pre>Model: SVC(C=10, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.96      0.97      0.97       225\n        True       0.54      0.47      0.50        15\n\n    accuracy                           0.94       240\n   macro avg       0.75      0.72      0.73       240\nweighted avg       0.94      0.94      0.94       240\n\nF1 score: 50.00%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\nBetter parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n</pre> <pre>Model: SVC(C=10, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.96      0.97      0.97       225\n        True       0.54      0.47      0.50        15\n\n    accuracy                           0.94       240\n   macro avg       0.75      0.72      0.73       240\nweighted avg       0.94      0.94      0.94       240\n\nF1 score: 50.00%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: Logistic Regression\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\nBetter parameters for Logistic Regression: {'C': 1, 'penalty': 'l1'}\n</pre> <pre>Model: LogisticRegression(C=1, max_iter=1000, n_jobs=-1, penalty='l1', solver='saga')\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97       225\n           1       0.60      0.60      0.60        15\n\n    accuracy                           0.95       240\n   macro avg       0.79      0.79      0.79       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 60.00%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: Logistic Regression\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\nBetter parameters for Logistic Regression: {'C': 0.1, 'penalty': 'l2'}\n</pre> <pre>Model: LogisticRegression(C=0.1, max_iter=1000, n_jobs=-1, solver='saga')\n              precision    recall  f1-score   support\n\n           0       1.00      0.94      0.97       225\n           1       0.52      1.00      0.68        15\n\n    accuracy                           0.94       240\n   macro avg       0.76      0.97      0.82       240\nweighted avg       0.97      0.94      0.95       240\n\nF1 score: 68.18%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: Logistic Regression\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n</pre> <pre>/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning:\n\nThe max_iter was reached which means the coef_ did not converge\n\n</pre> <pre>[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\nBetter parameters for Logistic Regression: {'C': 10, 'penalty': 'l1'}\n</pre> <pre>Model: LogisticRegression(C=10, max_iter=1000, n_jobs=-1, penalty='l1', solver='saga')\n              precision    recall  f1-score   support\n\n       False       0.97      0.97      0.97       225\n        True       0.56      0.60      0.58        15\n\n    accuracy                           0.95       240\n   macro avg       0.77      0.78      0.78       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 58.06%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: Logistic Regression\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n[CV] END ...................................C=10, penalty=l2; total time=   0.2s\n[CV] END ...................................C=10, penalty=l2; total time=   0.2s\nBetter parameters for Logistic Regression: {'C': 0.1, 'penalty': 'l2'}\n</pre> <pre>/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n</pre> <pre>Model: LogisticRegression(C=0.1, max_iter=1000, n_jobs=-1, solver='saga')\n              precision    recall  f1-score   support\n\n       False       1.00      0.94      0.97       225\n        True       0.54      1.00      0.70        15\n\n    accuracy                           0.95       240\n   macro avg       0.77      0.97      0.83       240\nweighted avg       0.97      0.95      0.95       240\n\nF1 score: 69.77%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: Extra Trees\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.4s\nBetter parameters for Extra Trees: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n</pre> <pre>Model: ExtraTreesClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.96      0.99      0.97       225\n           1       0.62      0.33      0.43        15\n\n    accuracy                           0.95       240\n   macro avg       0.79      0.66      0.70       240\nweighted avg       0.94      0.95      0.94       240\n\nF1 score: 43.48%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: Extra Trees\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\nBetter parameters for Extra Trees: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n</pre> <pre>Model: ExtraTreesClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.97      0.99      0.98       225\n           1       0.70      0.47      0.56        15\n\n    accuracy                           0.95       240\n   macro avg       0.83      0.73      0.77       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 56.00%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: Extra Trees\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.8s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.8s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.4s\nBetter parameters for Extra Trees: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n</pre> <pre>Model: ExtraTreesClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.96      1.00      0.98       225\n        True       0.86      0.40      0.55        15\n\n    accuracy                           0.96       240\n   macro avg       0.91      0.70      0.76       240\nweighted avg       0.95      0.96      0.95       240\n\nF1 score: 54.55%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: Extra Trees\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.9s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   1.0s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   1.0s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.9s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.8s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.8s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\nBetter parameters for Extra Trees: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n</pre> <pre>Model: ExtraTreesClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.96      1.00      0.98       225\n        True       0.83      0.33      0.48        15\n\n    accuracy                           0.95       240\n   macro avg       0.90      0.66      0.73       240\nweighted avg       0.95      0.95      0.94       240\n\nF1 score: 47.62%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: Gradient Boosting\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.1s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.1s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.1s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\nBetter parameters for Gradient Boosting: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300}\n</pre> <pre>Model: GradientBoostingClassifier(learning_rate=0.05, max_depth=2, n_estimators=300,\n                           random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97       225\n           1       0.53      0.53      0.53        15\n\n    accuracy                           0.94       240\n   macro avg       0.75      0.75      0.75       240\nweighted avg       0.94      0.94      0.94       240\n\nF1 score: 53.33%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: Gradient Boosting\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.1s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.1s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.6s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.6s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\nBetter parameters for Gradient Boosting: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300}\n</pre> <pre>Model: GradientBoostingClassifier(learning_rate=0.05, max_depth=2, n_estimators=300,\n                           random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       225\n           1       0.56      0.67      0.61        15\n\n    accuracy                           0.95       240\n   macro avg       0.77      0.82      0.79       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 60.61%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: Gradient Boosting\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.1s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\nBetter parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}\n</pre> <pre>Model: GradientBoostingClassifier(n_estimators=300, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.97      0.98      0.98       225\n        True       0.67      0.53      0.59        15\n\n    accuracy                           0.95       240\n   macro avg       0.82      0.76      0.78       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 59.26%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: Gradient Boosting\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.3s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.6s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.6s[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\nBetter parameters for Gradient Boosting: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300}\n</pre> <pre>Model: GradientBoostingClassifier(learning_rate=0.05, max_depth=2, n_estimators=300,\n                           random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.97      0.96      0.97       225\n        True       0.53      0.60      0.56        15\n\n    accuracy                           0.94       240\n   macro avg       0.75      0.78      0.77       240\nweighted avg       0.95      0.94      0.94       240\n\nF1 score: 56.25%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: HistGB\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\nBetter parameters for HistGB: {'learning_rate': 0.1, 'max_depth': None, 'max_leaf_nodes': 31}\n</pre> <pre>Model: HistGradientBoostingClassifier(random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97       225\n           1       0.60      0.60      0.60        15\n\n    accuracy                           0.95       240\n   macro avg       0.79      0.79      0.79       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 60.00%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: HistGB\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.3s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.3s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\nBetter parameters for HistGB: {'learning_rate': 0.05, 'max_depth': None, 'max_leaf_nodes': 31}\n</pre> <pre>Model: HistGradientBoostingClassifier(learning_rate=0.05, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.98       225\n           1       0.60      0.80      0.69        15\n\n    accuracy                           0.95       240\n   macro avg       0.79      0.88      0.83       240\nweighted avg       0.96      0.95      0.96       240\n\nF1 score: 68.57%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: HistGB\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.3s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\nBetter parameters for HistGB: {'learning_rate': 0.1, 'max_depth': None, 'max_leaf_nodes': 31}\n</pre> <pre>Model: HistGradientBoostingClassifier(random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.98      0.97      0.97       225\n        True       0.59      0.67      0.62        15\n\n    accuracy                           0.95       240\n   macro avg       0.78      0.82      0.80       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 62.50%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: HistGB\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.3s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.3s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.2s\nBetter parameters for HistGB: {'learning_rate': 0.05, 'max_depth': None, 'max_leaf_nodes': 31}\n</pre> <pre>Model: HistGradientBoostingClassifier(learning_rate=0.05, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.98      0.97      0.98       225\n        True       0.61      0.73      0.67        15\n\n    accuracy                           0.95       240\n   macro avg       0.80      0.85      0.82       240\nweighted avg       0.96      0.95      0.96       240\n\nF1 score: 66.67%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: KNN\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\nBetter parameters for KNN: {'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n</pre> <pre>Model: KNeighborsClassifier()\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96       225\n           1       0.43      0.40      0.41        15\n\n    accuracy                           0.93       240\n   macro avg       0.69      0.68      0.69       240\nweighted avg       0.93      0.93      0.93       240\n\nF1 score: 41.38%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: KNN\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\nBetter parameters for KNN: {'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n</pre> <pre>Model: KNeighborsClassifier(p=1, weights='distance')\n              precision    recall  f1-score   support\n\n           0       0.99      0.92      0.95       225\n           1       0.41      0.80      0.55        15\n\n    accuracy                           0.92       240\n   macro avg       0.70      0.86      0.75       240\nweighted avg       0.95      0.92      0.93       240\n\nF1 score: 54.55%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: KNN\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.1s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.1s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\nBetter parameters for KNN: {'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n</pre> <pre>Model: KNeighborsClassifier()\n              precision    recall  f1-score   support\n\n       False       0.97      0.96      0.97       225\n        True       0.50      0.53      0.52        15\n\n    accuracy                           0.94       240\n   macro avg       0.73      0.75      0.74       240\nweighted avg       0.94      0.94      0.94       240\n\nF1 score: 51.61%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: KNN\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\nBetter parameters for KNN: {'n_neighbors': 5, 'p': 2, 'weights': 'distance'}\n</pre> <pre>Model: KNeighborsClassifier(weights='distance')\n              precision    recall  f1-score   support\n\n       False       0.99      0.91      0.95       225\n        True       0.39      0.87      0.54        15\n\n    accuracy                           0.91       240\n   macro avg       0.69      0.89      0.75       240\nweighted avg       0.95      0.91      0.92       240\n\nF1 score: 54.17%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: Linear SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\nBetter parameters for Linear SVM: {'C': 10}\n</pre> <pre>Model: LinearSVC(C=10, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97       225\n           1       0.60      0.60      0.60        15\n\n    accuracy                           0.95       240\n   macro avg       0.79      0.79      0.79       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 60.00%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: Linear SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\nBetter parameters for Linear SVM: {'C': 0.1}\n</pre> <pre>Model: LinearSVC(C=0.1, random_state=42)\n              precision    recall  f1-score   support\n\n           0       1.00      0.93      0.97       225\n           1       0.50      1.00      0.67        15\n\n    accuracy                           0.94       240\n   macro avg       0.75      0.97      0.82       240\nweighted avg       0.97      0.94      0.95       240\n\nF1 score: 66.67%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: Linear SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\nBetter parameters for Linear SVM: {'C': 1}\n</pre> <pre>Model: LinearSVC(C=1, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.97      0.97      0.97       225\n        True       0.57      0.53      0.55        15\n\n    accuracy                           0.95       240\n   macro avg       0.77      0.75      0.76       240\nweighted avg       0.94      0.95      0.94       240\n\nF1 score: 55.17%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: Linear SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\nBetter parameters for Linear SVM: {'C': 0.1}\n</pre> <pre>Model: LinearSVC(C=0.1, random_state=42)\n              precision    recall  f1-score   support\n\n       False       1.00      0.93      0.96       225\n        True       0.48      1.00      0.65        15\n\n    accuracy                           0.93       240\n   macro avg       0.74      0.96      0.81       240\nweighted avg       0.97      0.93      0.94       240\n\nF1 score: 65.22%\n-------------------------------------------------------------------------------\n</pre> In\u00a0[216]: Copied! <pre>model = \"SVM\"\n\nprint('-------------------------- Original dataset -----------------------------------')\nprint(f'Model: {model}')\nprint('-------------------------------------------------------------------------------')\nX_train, X_test, y_train, y_test = preprocess(aux, drops, numerical_features, \"Fail\")\ntmodel = train_model(X_train, y_train, model)\npredict_and_evaluate(tmodel, X_test, y_test)\n</pre> model = \"SVM\"  print('-------------------------- Original dataset -----------------------------------') print(f'Model: {model}') print('-------------------------------------------------------------------------------') X_train, X_test, y_train, y_test = preprocess(aux, drops, numerical_features, \"Fail\") tmodel = train_model(X_train, y_train, model) predict_and_evaluate(tmodel, X_test, y_test) <pre>-------------------------- Original dataset -----------------------------------\nModel: SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\nBetter parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n</pre> <pre>Model: SVC(C=10, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98       225\n           1       0.63      0.80      0.71        15\n\n    accuracy                           0.96       240\n   macro avg       0.81      0.88      0.84       240\nweighted avg       0.96      0.96      0.96       240\n\nF1 score: 70.59%\n</pre> In\u00a0[219]: Copied! <pre>X_train, X_test, y_train, y_test = preprocess(aux, drops, numerical_features, \"Fail\")\n\ntmodel = RandomForestClassifier(max_depth=10, min_samples_split=10, \n                                n_estimators=300, random_state=42)\n\ntmodel.fit(X_train, y_train)\n\npredict_and_evaluate(tmodel, X_test, y_test)\n\ndef plot_feature_importance(model, X_train):\n\n    importances = model.feature_importances_\n\n    feature_names = X_train.columns\n\n    feature_importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances\n    }).sort_values(by='Importance', ascending=False)\n\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='royalblue')\n    plt.xlabel('Importance')\n    plt.ylabel('Feature')\n    plt.title('Feature importance - Random Forest')\n    plt.gca().invert_yaxis()\n    plt.show()\n\nplot_feature_importance(tmodel, X_train)\n</pre> X_train, X_test, y_train, y_test = preprocess(aux, drops, numerical_features, \"Fail\")  tmodel = RandomForestClassifier(max_depth=10, min_samples_split=10,                                  n_estimators=300, random_state=42)  tmodel.fit(X_train, y_train)  predict_and_evaluate(tmodel, X_test, y_test)  def plot_feature_importance(model, X_train):      importances = model.feature_importances_      feature_names = X_train.columns      feature_importance_df = pd.DataFrame({         'Feature': feature_names,         'Importance': importances     }).sort_values(by='Importance', ascending=False)      plt.figure(figsize=(10, 6))     plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='royalblue')     plt.xlabel('Importance')     plt.ylabel('Feature')     plt.title('Feature importance - Random Forest')     plt.gca().invert_yaxis()     plt.show()  plot_feature_importance(tmodel, X_train)  <pre>Model: RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=300,\n                       random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97       225\n           1       0.64      0.47      0.54        15\n\n    accuracy                           0.95       240\n   macro avg       0.80      0.72      0.76       240\nweighted avg       0.94      0.95      0.95       240\n\nF1 score: 53.85%\n</pre>"},{"location":"answers/#major-task","title":"Major task\u00b6","text":"<ul> <li>Investigate one piece of equipment in different time cycles to understand what characteristics and parameters of the sensors might indicate that the equipment is on the verge of failing.</li> </ul>"},{"location":"answers/#task-1","title":"Task 1\u00b6","text":"<ul> <li>Calculate how many times the equipment has failed.</li> </ul> <p>During the FPSO\u2019s operation, various factors can cause the machine to fail and prolong its failure state.</p> <p>We ask you to explore the available data, identify, and calculate the number of times the equipment has failed throughout its operation.</p>"},{"location":"answers/#task-2","title":"Task 2\u00b6","text":"<ul> <li><p>Categorize equipment failures by setup configurations (Preset 1 and Preset 2).</p> </li> <li><p>How do the variables Preset_1 and Preset_2 behave during operation?</p> </li> <li><p>What insights can we derive from these variables?</p> </li> </ul>"},{"location":"answers/#evaluating-presets-regarding-number-of-obsertations-and-failure-percentage","title":"Evaluating Presets regarding Number of obsertations and Failure percentage\u00b6","text":""},{"location":"answers/#presets-failure-percentage-heatmap","title":"Presets Failure Percentage Heatmap\u00b6","text":""},{"location":"answers/#presets-state-changes-during-operation","title":"Presets state changes during operation\u00b6","text":""},{"location":"answers/#boxplots-per-presets-during-operation","title":"Boxplots per presets during operation\u00b6","text":""},{"location":"answers/#task-3","title":"Task 3\u00b6","text":"<ul> <li><p>Categorize equipment failures by their nature/root cause according to parameter readings (temperature, pressure, and others).</p> </li> <li><p>Analyze patterns in these readings that could indicate specific failure types.</p> </li> <li><p>How do these patterns differ across operational regimes?</p> </li> <li><p>Provide insights based on your findings.</p> </li> </ul>"},{"location":"answers/#task-4","title":"Task 4\u00b6","text":"<ul> <li><p>Create a model (or models) using the technique you think is most appropriate and measure its performance.</p> </li> <li><p>Based on the given time-series dataset, which models or techniques are suitable for predicting whether the equipment will fail before it occurs?</p> </li> <li><p>Additionally, how can the model's performance be tuned and measured for this task?</p> </li> </ul>"},{"location":"answers/#model-choice","title":"Model choice\u00b6","text":""},{"location":"answers/#task-5","title":"Task 5\u00b6","text":"<ul> <li><p>Analyze variable importance.</p> </li> <li><p>After developing a model, how can we determine which variables had the greatest impact on the prediction?</p> </li> </ul>"}]}