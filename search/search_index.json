{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"answers/","title":"Case","text":"<p>nome: Rafael Marinho Ferreira</p> <p>email: rafamarinho87@gmail.com</p> <p>github: https://github.com/rferreira13</p> In\u00a0[220]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport scipy.stats as st\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.tsa.stattools import adfuller\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import make_scorer, recall_score\n</pre> import pandas as pd import numpy as np import scipy.stats as st import seaborn as sns import matplotlib.pyplot as plt import plotly.express as px from plotly.subplots import make_subplots import plotly.graph_objects as go  import statsmodels.api as sm import statsmodels.formula.api as smf from statsmodels.tsa.stattools import adfuller  from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier from sklearn.svm import SVC, LinearSVC from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from imblearn.over_sampling import RandomOverSampler from sklearn.model_selection import GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report, confusion_matrix, f1_score from sklearn.model_selection import train_test_split from sklearn.metrics import make_scorer, recall_score In\u00a0[221]: Copied! <pre>df = pd.read_excel(\"Test O_G_Equipment_Data.xlsx\")\n\ndf.to_csv(\"data.csv\", index=False)\n</pre> df = pd.read_excel(\"Test O_G_Equipment_Data.xlsx\")  df.to_csv(\"data.csv\", index=False) In\u00a0[222]: Copied! <pre>fig = px.bar(df[\"Fail\"].value_counts().reset_index(), x='Fail', y='count', text_auto='s')\nfig.update_traces(textposition=\"outside\")\nfig.update_layout(\n        title=dict(\n            text=\"Count of Fail and Not Fail Events\",\n            x=0.5, xanchor='center',\n            y=0.94, yanchor='top',\n            font=dict(size=26),\n            pad=dict(t=10, b=10)\n        ),\n        margin=dict(l=60, r=40, t=120, b=50)\n    )\nfig.update_yaxes(range=[0, df.shape[0]])\nfig.show()\n</pre> fig = px.bar(df[\"Fail\"].value_counts().reset_index(), x='Fail', y='count', text_auto='s') fig.update_traces(textposition=\"outside\") fig.update_layout(         title=dict(             text=\"Count of Fail and Not Fail Events\",             x=0.5, xanchor='center',             y=0.94, yanchor='top',             font=dict(size=26),             pad=dict(t=10, b=10)         ),         margin=dict(l=60, r=40, t=120, b=50)     ) fig.update_yaxes(range=[0, df.shape[0]]) fig.show() In\u00a0[223]: Copied! <pre>\"{:.2%}\".format(df['Fail'].mean())\n</pre> \"{:.2%}\".format(df['Fail'].mean()) Out[223]: <pre>'8.25%'</pre> In\u00a0[224]: Copied! <pre>def failure_report(data, group):\n    data_copy = data.copy()\n\n    total = (data_copy[group].astype(str)\n            .value_counts().rename_axis(group)\n            .reset_index(name='Number of observations'))\n\n    taxa = (\n        data_copy[data_copy[\"Fail\"]][group].astype(str).value_counts() \n        / data_copy[group].astype(str).value_counts()\n        ).reset_index(name='Failure percentage')\n\n    base = (total.merge(taxa, on=group, how='left')\n                .fillna({'Failure percentage': 0})\n                .sort_values(group))\n\n    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n    fig.add_trace(\n        go.Bar(x=base[group], y=base['Number of observations'],\n            name='Number of observations',\n            text=base['Number of observations'], textposition='outside'),\n        secondary_y=False\n    )\n\n    fig.add_trace(\n        go.Scatter(x=base[group], y=base['Failure percentage'],\n                name='Failure percentage',\n                mode='lines+markers+text',\n                line=dict(color='red', width=2),\n                text=base['Failure percentage'],\n                texttemplate='%{y:.1%}',\n                textposition='top center',\n                cliponaxis=False),\n        secondary_y=True\n    )\n\n    fig.update_yaxes(title_text='Number of observations', secondary_y=False)\n    fig.update_yaxes(title_text='Failure (%)', tickformat='.1%', secondary_y=True)\n    fig.update_xaxes(title_text=group)\n    fig.update_layout(\n        title=dict(\n            text=f\"Number of observations versus Failure percentage&lt;br&gt;&lt;span style='font-size:16px;'&gt;Considering {group}&lt;/span&gt;\",\n            x=0.5, xanchor='center',\n            y=0.94, yanchor='top',\n            font=dict(size=26),\n            pad=dict(t=10, b=10)\n        ),\n        margin=dict(l=60, r=40, t=120, b=50)\n    )\n    fig.show()\n\ndef heatmap_failure(data):\n\n    data_copy = data.copy()\n\n    matrix_data = (data_copy.groupby(['Preset_1','Preset_2'])['Fail'].mean()\n        .mul(100).unstack(fill_value=0).sort_index())\n\n    matrix_data.columns = matrix_data.columns.astype(str)\n    matrix_data.index = matrix_data.index.astype(str)\n\n    fig = px.imshow(matrix_data, text_auto='.1f', aspect='auto',\n            color_continuous_scale='Reds',\n            labels=dict(color='Fail %')).update_layout(\n        xaxis_title='Preset_2', yaxis_title='Preset_1'\n    )\n\n    fig.update_layout(\n        title=dict(\n            text=\"Percentage Failure Heatmap&lt;br&gt;&lt;span style='font-size:16px;'&gt;Combining Preset_1 and Preset_2&lt;/span&gt;\",\n            x=0.5, xanchor='center',\n            y=0.94, yanchor='top',\n            font=dict(size=26),\n            pad=dict(t=10, b=10)\n        ),\n        margin=dict(l=60, r=40, t=120, b=50)\n    )\n    \n    \n    fig.show()\n\ndef heatmap_configuration_changes(data, group):\n\n    data_copy = data.copy()\n\n    data_copy[f'Previous {group}'] = data_copy[group].shift()\n\n    data_copy = data_copy.dropna()\n\n    data_copy[f'Previous {group}'] = data_copy[f'Previous {group}'].astype(int)\n\n    data_copy[f'Previous {group}'] = data_copy[f'Previous {group}'].astype(str)\n\n    data_copy[group] = data_copy[group].astype(str)\n\n    n_changes_df = (\n        data_copy\n        [[f'Previous {group}', group]]\n        .value_counts()\n        .reset_index(name=\"Number of Changes\")\n    )\n\n    n_changes_df[\"Percent of changes during operation\"] = n_changes_df[\"Number of Changes\"] / data_copy.shape[0]\n\n    n_changes_df = (\n        n_changes_df\n        .rename(columns={\n            f'Previous {group}':\"From\",\n            group:\"To\"\n            }\n        )\n        .sort_values(by=[\"From\", \"To\"])\n        .reset_index(drop=True)\n    )\n\n    order_x = sorted(n_changes_df[\"From\"].unique())\n    order_y = sorted(n_changes_df[\"To\"].unique())\n\n    fig = px.density_heatmap(\n        n_changes_df,\n        x=\"From\", y=\"To\", z=\"Percent of changes during operation\",\n        histfunc=\"sum\",\n        text_auto=True,\n        category_orders={\"From\": order_x, \"To\": order_y},\n        labels={\"From\": \"From\", \"To\": \"To\", \"Percent of changes during operation\": \"Percent of changes during operation\"},\n        color_continuous_scale=\"Reds\"\n    )\n\n    fig.update_traces(\n        meta = np.array([[(n_changes_df.loc[(n_changes_df[\"From\"]==x) &amp; (n_changes_df[\"To\"]==y),\n                                       \"Number of Changes\"].values[0]) if (n_changes_df.loc[(n_changes_df[\"From\"]==x) &amp; (n_changes_df[\"To\"]==y),\n                                       \"Number of Changes\"].values) else 0\n                      for x in order_x] for y in order_y[::-1]]),\n        hovertemplate = \"From=%{x}&lt;br&gt;To=%{y}&lt;br&gt;Percent=%{z:.2%}&lt;br&gt;Number of Changes=%{meta}&lt;extra&gt;&lt;/extra&gt;\",\n        texttemplate=\"%{z:.2%}\"\n    )\n    fig.update_layout(\n        coloraxis_colorbar=dict(\n            title=\"Percent of changes during operation\",\n            tickformat=\".2%\"\n        )\n    )\n\n    fig.update_layout(\n            title=dict(\n                text=f\"Percent of changes from {group} configurations during operation\",\n                x=0.5, xanchor='center',\n                y=0.94, yanchor='top',\n                font=dict(size=26),\n                pad=dict(t=10, b=10)\n            ),\n            margin=dict(l=60, r=40, t=120, b=50)\n        )\n    if n_changes_df.shape[0] &gt; 20:\n        fig.update_layout(height=900)\n\n    fig.show()\n\ndef boxplots(data, group, variable):\n\n    data_copy = data.copy()\n\n    model = smf.ols(f'{variable} ~ C({group})', data=data_copy).fit()\n    resid = model.resid\n    fitted = model.fittedvalues\n\n\n    fig = st.probplot(resid, dist=\"norm\", plot=plt)\n    plt.title(f\"QQ plot of residuals - {variable} ~ C({group})\")\n    plt.show()\n\n\n    plt.scatter(fitted, resid, s=8, alpha=0.6)\n    plt.axhline(0, lw=1, color='k')\n    plt.xlabel(\"Fitted values\")\n    plt.ylabel(\"Residuals\")\n    plt.title(f\"Residuals vs Fitted - {variable} ~ C({group})\")\n    plt.show()\n\n    fig = px.box(data_copy, x=group, y=variable)\n    fig.update_layout(\n        xaxis_title=group, yaxis_title=variable,\n        xaxis_tickangle=45, margin=dict(t=120, b=100)\n    )\n\n    model = smf.ols(f'{variable} ~ C({group})', data=data_copy).fit()\n    anova_tbl = sm.stats.anova_lm(model, typ=2)\n\n    df1 = float(anova_tbl.loc[f'C({group})', 'df'])\n    df2 = float(anova_tbl.loc['Residual', 'df'])\n    Fval = float(anova_tbl.loc[f'C({group})', 'F'])\n    pval = float(anova_tbl.loc[f'C({group})', 'PR(&gt;F)'])\n    ss_effect = float(anova_tbl.loc[f'C({group})', 'sum_sq'])\n    ss_error  = float(anova_tbl.loc['Residual', 'sum_sq'])\n    mse = ss_error / df2\n\n    y = data_copy[variable].to_numpy(dtype=float)\n    ss_total = float(((y - y.mean())**2).sum())\n\n    eta2   = ss_effect / ss_total\n    omega2 = (ss_effect - df1 * mse) / (ss_total + mse)\n\n    k = data_copy[group].nunique()\n    N = len(data_copy)\n\n\n    text_ = (\n        f\"ANOVA (one-way) \u2014 {variable} ~ {group}&lt;br&gt;\"\n        f\"F({int(df1)}, {int(df2)}) = {Fval:.2f}, p = {pval:.3g} \"\n        f\"| \u03b7\u00b2 = {eta2:.3f}, \u03c9\u00b2 = {omega2:.3f} \"\n        f\"| k = {k}, N = {N}\"\n    )\n\n    fig.add_annotation(\n        xref='paper', yref='paper', x=0.0, y=1.12,\n        text=text_, showarrow=False, align='left',\n        bgcolor='rgba(255,255,255,0.75)', bordercolor='black', borderwidth=1,\n        font=dict(size=12)\n    )\n\n    fig.show()\n</pre> def failure_report(data, group):     data_copy = data.copy()      total = (data_copy[group].astype(str)             .value_counts().rename_axis(group)             .reset_index(name='Number of observations'))      taxa = (         data_copy[data_copy[\"Fail\"]][group].astype(str).value_counts()          / data_copy[group].astype(str).value_counts()         ).reset_index(name='Failure percentage')      base = (total.merge(taxa, on=group, how='left')                 .fillna({'Failure percentage': 0})                 .sort_values(group))      fig = make_subplots(specs=[[{\"secondary_y\": True}]])      fig.add_trace(         go.Bar(x=base[group], y=base['Number of observations'],             name='Number of observations',             text=base['Number of observations'], textposition='outside'),         secondary_y=False     )      fig.add_trace(         go.Scatter(x=base[group], y=base['Failure percentage'],                 name='Failure percentage',                 mode='lines+markers+text',                 line=dict(color='red', width=2),                 text=base['Failure percentage'],                 texttemplate='%{y:.1%}',                 textposition='top center',                 cliponaxis=False),         secondary_y=True     )      fig.update_yaxes(title_text='Number of observations', secondary_y=False)     fig.update_yaxes(title_text='Failure (%)', tickformat='.1%', secondary_y=True)     fig.update_xaxes(title_text=group)     fig.update_layout(         title=dict(             text=f\"Number of observations versus Failure percentageConsidering {group}\",             x=0.5, xanchor='center',             y=0.94, yanchor='top',             font=dict(size=26),             pad=dict(t=10, b=10)         ),         margin=dict(l=60, r=40, t=120, b=50)     )     fig.show()  def heatmap_failure(data):      data_copy = data.copy()      matrix_data = (data_copy.groupby(['Preset_1','Preset_2'])['Fail'].mean()         .mul(100).unstack(fill_value=0).sort_index())      matrix_data.columns = matrix_data.columns.astype(str)     matrix_data.index = matrix_data.index.astype(str)      fig = px.imshow(matrix_data, text_auto='.1f', aspect='auto',             color_continuous_scale='Reds',             labels=dict(color='Fail %')).update_layout(         xaxis_title='Preset_2', yaxis_title='Preset_1'     )      fig.update_layout(         title=dict(             text=\"Percentage Failure HeatmapCombining Preset_1 and Preset_2\",             x=0.5, xanchor='center',             y=0.94, yanchor='top',             font=dict(size=26),             pad=dict(t=10, b=10)         ),         margin=dict(l=60, r=40, t=120, b=50)     )               fig.show()  def heatmap_configuration_changes(data, group):      data_copy = data.copy()      data_copy[f'Previous {group}'] = data_copy[group].shift()      data_copy = data_copy.dropna()      data_copy[f'Previous {group}'] = data_copy[f'Previous {group}'].astype(int)      data_copy[f'Previous {group}'] = data_copy[f'Previous {group}'].astype(str)      data_copy[group] = data_copy[group].astype(str)      n_changes_df = (         data_copy         [[f'Previous {group}', group]]         .value_counts()         .reset_index(name=\"Number of Changes\")     )      n_changes_df[\"Percent of changes during operation\"] = n_changes_df[\"Number of Changes\"] / data_copy.shape[0]      n_changes_df = (         n_changes_df         .rename(columns={             f'Previous {group}':\"From\",             group:\"To\"             }         )         .sort_values(by=[\"From\", \"To\"])         .reset_index(drop=True)     )      order_x = sorted(n_changes_df[\"From\"].unique())     order_y = sorted(n_changes_df[\"To\"].unique())      fig = px.density_heatmap(         n_changes_df,         x=\"From\", y=\"To\", z=\"Percent of changes during operation\",         histfunc=\"sum\",         text_auto=True,         category_orders={\"From\": order_x, \"To\": order_y},         labels={\"From\": \"From\", \"To\": \"To\", \"Percent of changes during operation\": \"Percent of changes during operation\"},         color_continuous_scale=\"Reds\"     )      fig.update_traces(         meta = np.array([[(n_changes_df.loc[(n_changes_df[\"From\"]==x) &amp; (n_changes_df[\"To\"]==y),                                        \"Number of Changes\"].values[0]) if (n_changes_df.loc[(n_changes_df[\"From\"]==x) &amp; (n_changes_df[\"To\"]==y),                                        \"Number of Changes\"].values) else 0                       for x in order_x] for y in order_y[::-1]]),         hovertemplate = \"From=%{x}To=%{y}Percent=%{z:.2%}Number of Changes=%{meta}\",         texttemplate=\"%{z:.2%}\"     )     fig.update_layout(         coloraxis_colorbar=dict(             title=\"Percent of changes during operation\",             tickformat=\".2%\"         )     )      fig.update_layout(             title=dict(                 text=f\"Percent of changes from {group} configurations during operation\",                 x=0.5, xanchor='center',                 y=0.94, yanchor='top',                 font=dict(size=26),                 pad=dict(t=10, b=10)             ),             margin=dict(l=60, r=40, t=120, b=50)         )     if n_changes_df.shape[0] &gt; 20:         fig.update_layout(height=900)      fig.show()  def boxplots(data, group, variable):      data_copy = data.copy()      model = smf.ols(f'{variable} ~ C({group})', data=data_copy).fit()     resid = model.resid     fitted = model.fittedvalues       fig = st.probplot(resid, dist=\"norm\", plot=plt)     plt.title(f\"QQ plot of residuals - {variable} ~ C({group})\")     plt.show()       plt.scatter(fitted, resid, s=8, alpha=0.6)     plt.axhline(0, lw=1, color='k')     plt.xlabel(\"Fitted values\")     plt.ylabel(\"Residuals\")     plt.title(f\"Residuals vs Fitted - {variable} ~ C({group})\")     plt.show()      fig = px.box(data_copy, x=group, y=variable)     fig.update_layout(         xaxis_title=group, yaxis_title=variable,         xaxis_tickangle=45, margin=dict(t=120, b=100)     )      model = smf.ols(f'{variable} ~ C({group})', data=data_copy).fit()     anova_tbl = sm.stats.anova_lm(model, typ=2)      df1 = float(anova_tbl.loc[f'C({group})', 'df'])     df2 = float(anova_tbl.loc['Residual', 'df'])     Fval = float(anova_tbl.loc[f'C({group})', 'F'])     pval = float(anova_tbl.loc[f'C({group})', 'PR(&gt;F)'])     ss_effect = float(anova_tbl.loc[f'C({group})', 'sum_sq'])     ss_error  = float(anova_tbl.loc['Residual', 'sum_sq'])     mse = ss_error / df2      y = data_copy[variable].to_numpy(dtype=float)     ss_total = float(((y - y.mean())**2).sum())      eta2   = ss_effect / ss_total     omega2 = (ss_effect - df1 * mse) / (ss_total + mse)      k = data_copy[group].nunique()     N = len(data_copy)       text_ = (         f\"ANOVA (one-way) \u2014 {variable} ~ {group}\"         f\"F({int(df1)}, {int(df2)}) = {Fval:.2f}, p = {pval:.3g} \"         f\"| \u03b7\u00b2 = {eta2:.3f}, \u03c9\u00b2 = {omega2:.3f} \"         f\"| k = {k}, N = {N}\"     )      fig.add_annotation(         xref='paper', yref='paper', x=0.0, y=1.12,         text=text_, showarrow=False, align='left',         bgcolor='rgba(255,255,255,0.75)', bordercolor='black', borderwidth=1,         font=dict(size=12)     )      fig.show() In\u00a0[225]: Copied! <pre>failure_report(df, \"Preset_1\")\nfailure_report(df, \"Preset_2\")\n\naux_data = df.copy()\naux_data[\"Preset_1_2\"] = df['Preset_1'].astype(str) + df['Preset_2'].astype(str)\n\nfailure_report(aux_data, \"Preset_1_2\")\n</pre> failure_report(df, \"Preset_1\") failure_report(df, \"Preset_2\")  aux_data = df.copy() aux_data[\"Preset_1_2\"] = df['Preset_1'].astype(str) + df['Preset_2'].astype(str)  failure_report(aux_data, \"Preset_1_2\") In\u00a0[226]: Copied! <pre>heatmap_failure(df)\n</pre> heatmap_failure(df) In\u00a0[227]: Copied! <pre>heatmap_configuration_changes(df, \"Preset_1\")\nheatmap_configuration_changes(df, \"Preset_2\")\n\naux_data = df.copy()\naux_data[\"Preset_1_2\"] = df['Preset_1'].astype(str) + df['Preset_2'].astype(str)\n\nheatmap_configuration_changes(aux_data, \"Preset_1_2\")\n</pre> heatmap_configuration_changes(df, \"Preset_1\") heatmap_configuration_changes(df, \"Preset_2\")  aux_data = df.copy() aux_data[\"Preset_1_2\"] = df['Preset_1'].astype(str) + df['Preset_2'].astype(str)  heatmap_configuration_changes(aux_data, \"Preset_1_2\") <pre>/tmp/ipykernel_710/388047762.py:131: DeprecationWarning:\n\nThe truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &gt; 0` to check that an array is not empty.\n\n</pre> In\u00a0[228]: Copied! <pre>for column in df.columns[3:-1]:\n    boxplots(df, 'Preset_1', column)\n\nfor column in df.columns[3:-1]:\n    boxplots(df, 'Preset_2', column)\n\naux = df.copy()\naux['Preset_1_2'] = aux['Preset_1'].astype(str) + '-' + aux['Preset_2'].astype(str)\nfor column in df.columns[3:-1]:\n    boxplots(aux, 'Preset_1_2', column)\n</pre> for column in df.columns[3:-1]:     boxplots(df, 'Preset_1', column)  for column in df.columns[3:-1]:     boxplots(df, 'Preset_2', column)  aux = df.copy() aux['Preset_1_2'] = aux['Preset_1'].astype(str) + '-' + aux['Preset_2'].astype(str) for column in df.columns[3:-1]:     boxplots(aux, 'Preset_1_2', column) In\u00a0[229]: Copied! <pre>def cycles_per_variable(data, x_axis, variable):\n\n    data_copy = data.copy()\n\n    data_copy[variable].quantile(0.95)\n    data_copy[variable].quantile(0.05)\n\n    p05 = data_copy[variable].quantile(0.05)\n    p95 = data_copy[variable].quantile(0.95)\n    median = data_copy[variable].median()\n\n\n\n    fig = px.line(\n        data_copy.sort_values([x_axis]),\n        x=x_axis, y=variable\n    )\n\n    fig.add_trace(go.Scatter(\n        x=data_copy[x_axis], y=[p95] * len(data_copy),\n        mode='lines',\n        line=dict(width=0),\n        showlegend=False,\n        hoverinfo='skip'\n    ))\n    fig.add_trace(go.Scatter(\n        x=data_copy[x_axis], y=[p05] * len(data_copy),\n        mode='lines',\n        line=dict(width=0),\n        fill='tonexty',\n        name='P05\u2013P95',\n        opacity=0.2,\n        hoverinfo='skip'\n    ))\n    fig.add_trace(go.Scatter(\n        x=data_copy[x_axis], y=[median] * len(data_copy),\n        mode='lines',\n        line=dict(width=2, dash=\"dash\", color=\"blue\"),\n        name='Median',\n        hoverinfo='skip'\n    ))\n\n\n    fig.add_hline(y=p95, line_dash='dot', line_width=1, annotation_text='P95', annotation_position='top left')\n    fig.add_hline(y=p05, line_dash='dot', line_width=1, annotation_text='P05', annotation_position='bottom left')\n\n    # fig.add_hline(y=median, line_dash='dash', line_width=1.5, annotation_text='Median', annotation_position='top right')\n\n    fig.data[0].update(name=variable, showlegend=True, line=dict(color=\"red\", width=2))\n\n    for x in data_copy.loc[data_copy[\"Fail\"], x_axis].unique():\n        fig.add_vline(x=x, line_width=2, line_color=\"rgba(0,0,0,0.5)\")\n\n    fig.add_scatter(x=[None], y=[None], mode=\"lines\",\n                    line=dict(color=\"rgba(0,0,0,0.5)\", width=1),\n                    name=\"Failure events\")\n\n    fig.update_yaxes(range=[0, 1.1 * data_copy[variable].max()])\n\n    fig.update_layout(\n        title=dict(\n            text=f\"{variable} per {x_axis} with Failure events\",\n            x=0.5, xanchor='center',\n            y=0.94, yanchor='top',\n            font=dict(size=26),\n            pad=dict(t=10, b=10)\n        ),\n        margin=dict(l=60, r=40, t=120, b=50)\n    )\n\n    fig.show()\n\ndef failure_3d_plot(data, x_value, y_value, z_value):\n\n    data_copy = data.copy()\n\n    fig = px.scatter_3d(\n        data_copy, x=x_value, y=y_value, z=z_value,\n        color=\"Fail\",\n        color_discrete_map={True: \"black\", False: \"lightgray\"}\n    )\n\n    fig.update_traces(marker=dict(size=3))\n\n\n    fig.update_layout(\n        title=dict(\n            text=f\"3d Plot {x_value} versus {y_value} versus {z_value} per Failure event\",\n            x=0.5, xanchor='center',\n            y=0.94, yanchor='top',\n            font=dict(size=26),\n            pad=dict(t=10, b=10)\n        ),\n        margin=dict(l=60, r=40, t=120, b=50)\n    )\n    fig.show()\n\ndef failure_2d_plot(data, x_value, y_value):\n\n    data_copy = data.copy()\n\n    fig = px.scatter(\n            data_copy,\n            x=x_value, y=y_value,\n            color=\"Fail\"\n        )\n\n    fig.update_layout(\n        title=dict(\n            text=f\"Scatter Plot {x_value} versus {y_value} per Failure event\",\n            x=0.5, xanchor='center',\n            y=0.94, yanchor='top',\n            font=dict(size=26),\n            pad=dict(t=10, b=10)\n        ),\n        margin=dict(l=60, r=40, t=120, b=50)\n    )\n\n    fig.show()\n</pre> def cycles_per_variable(data, x_axis, variable):      data_copy = data.copy()      data_copy[variable].quantile(0.95)     data_copy[variable].quantile(0.05)      p05 = data_copy[variable].quantile(0.05)     p95 = data_copy[variable].quantile(0.95)     median = data_copy[variable].median()        fig = px.line(         data_copy.sort_values([x_axis]),         x=x_axis, y=variable     )      fig.add_trace(go.Scatter(         x=data_copy[x_axis], y=[p95] * len(data_copy),         mode='lines',         line=dict(width=0),         showlegend=False,         hoverinfo='skip'     ))     fig.add_trace(go.Scatter(         x=data_copy[x_axis], y=[p05] * len(data_copy),         mode='lines',         line=dict(width=0),         fill='tonexty',         name='P05\u2013P95',         opacity=0.2,         hoverinfo='skip'     ))     fig.add_trace(go.Scatter(         x=data_copy[x_axis], y=[median] * len(data_copy),         mode='lines',         line=dict(width=2, dash=\"dash\", color=\"blue\"),         name='Median',         hoverinfo='skip'     ))       fig.add_hline(y=p95, line_dash='dot', line_width=1, annotation_text='P95', annotation_position='top left')     fig.add_hline(y=p05, line_dash='dot', line_width=1, annotation_text='P05', annotation_position='bottom left')      # fig.add_hline(y=median, line_dash='dash', line_width=1.5, annotation_text='Median', annotation_position='top right')      fig.data[0].update(name=variable, showlegend=True, line=dict(color=\"red\", width=2))      for x in data_copy.loc[data_copy[\"Fail\"], x_axis].unique():         fig.add_vline(x=x, line_width=2, line_color=\"rgba(0,0,0,0.5)\")      fig.add_scatter(x=[None], y=[None], mode=\"lines\",                     line=dict(color=\"rgba(0,0,0,0.5)\", width=1),                     name=\"Failure events\")      fig.update_yaxes(range=[0, 1.1 * data_copy[variable].max()])      fig.update_layout(         title=dict(             text=f\"{variable} per {x_axis} with Failure events\",             x=0.5, xanchor='center',             y=0.94, yanchor='top',             font=dict(size=26),             pad=dict(t=10, b=10)         ),         margin=dict(l=60, r=40, t=120, b=50)     )      fig.show()  def failure_3d_plot(data, x_value, y_value, z_value):      data_copy = data.copy()      fig = px.scatter_3d(         data_copy, x=x_value, y=y_value, z=z_value,         color=\"Fail\",         color_discrete_map={True: \"black\", False: \"lightgray\"}     )      fig.update_traces(marker=dict(size=3))       fig.update_layout(         title=dict(             text=f\"3d Plot {x_value} versus {y_value} versus {z_value} per Failure event\",             x=0.5, xanchor='center',             y=0.94, yanchor='top',             font=dict(size=26),             pad=dict(t=10, b=10)         ),         margin=dict(l=60, r=40, t=120, b=50)     )     fig.show()  def failure_2d_plot(data, x_value, y_value):      data_copy = data.copy()      fig = px.scatter(             data_copy,             x=x_value, y=y_value,             color=\"Fail\"         )      fig.update_layout(         title=dict(             text=f\"Scatter Plot {x_value} versus {y_value} per Failure event\",             x=0.5, xanchor='center',             y=0.94, yanchor='top',             font=dict(size=26),             pad=dict(t=10, b=10)         ),         margin=dict(l=60, r=40, t=120, b=50)     )      fig.show() In\u00a0[230]: Copied! <pre>for column in df.columns[3:-1]:\n    cycles_per_variable(df, \"Cycle\", column)\n\naux_data = df.copy()\n\naux_data[\"Module Vibration\"] = aux_data.apply(lambda x: ((x[\"VibrationX\"]**2) + (x[\"VibrationY\"]**2) + (x[\"VibrationZ\"]**2))**(1/2), axis=1)\n\ncycles_per_variable(aux_data, \"Cycle\", \"Module Vibration\")\n\nfailure_2d_plot(df, \"Temperature\", \"Pressure\")\n\nfailure_2d_plot(aux_data, \"Module Vibration\", \"Frequency\")\n\nfailure_2d_plot(aux_data, \"Module Vibration\", \"Temperature\")\n\nfailure_2d_plot(aux_data, \"Module Vibration\", \"Pressure\")\n\nfailure_3d_plot(df, \"Temperature\", \"Pressure\", \"Frequency\")\n\nfailure_3d_plot(df, \"VibrationX\", \"VibrationY\", \"VibrationZ\")\n</pre> for column in df.columns[3:-1]:     cycles_per_variable(df, \"Cycle\", column)  aux_data = df.copy()  aux_data[\"Module Vibration\"] = aux_data.apply(lambda x: ((x[\"VibrationX\"]**2) + (x[\"VibrationY\"]**2) + (x[\"VibrationZ\"]**2))**(1/2), axis=1)  cycles_per_variable(aux_data, \"Cycle\", \"Module Vibration\")  failure_2d_plot(df, \"Temperature\", \"Pressure\")  failure_2d_plot(aux_data, \"Module Vibration\", \"Frequency\")  failure_2d_plot(aux_data, \"Module Vibration\", \"Temperature\")  failure_2d_plot(aux_data, \"Module Vibration\", \"Pressure\")  failure_3d_plot(df, \"Temperature\", \"Pressure\", \"Frequency\")  failure_3d_plot(df, \"VibrationX\", \"VibrationY\", \"VibrationZ\") In\u00a0[231]: Copied! <pre>def test_stationarity(series, scope):\n\n    print(f'Dickey-Fuller (Dataset {scope}):')\n    adf_test = adfuller(series.dropna())\n\n    adf_stat = adf_test[0]\n    p_value = adf_test[1]\n    critical_values = adf_test[4]\n\n\n    print(f'ADF statistic: {adf_stat}')\n    print(f'p-value: {p_value}')\n    print('Critical values:')\n    for key, value in critical_values.items():\n        print(f'\\t{key}: {value}')\n\n\n    if adf_stat &lt; critical_values['5%']:\n        print(\"The series is stationary (Reject the null hypothesis)\")\n    else:\n        print(\"The series is NOT stationary (Fail to reject the null hypothesis)\")\n\n\nnumerical_features = [\"Temperature\", \"Pressure\", \"VibrationX\", \"VibrationY\", \"VibrationZ\", \"Frequency\"]\nfor col in numerical_features:\n    s = df.sort_values('Cycle')[col]\n    test_stationarity(s, col)\n</pre> def test_stationarity(series, scope):      print(f'Dickey-Fuller (Dataset {scope}):')     adf_test = adfuller(series.dropna())      adf_stat = adf_test[0]     p_value = adf_test[1]     critical_values = adf_test[4]       print(f'ADF statistic: {adf_stat}')     print(f'p-value: {p_value}')     print('Critical values:')     for key, value in critical_values.items():         print(f'\\t{key}: {value}')       if adf_stat &lt; critical_values['5%']:         print(\"The series is stationary (Reject the null hypothesis)\")     else:         print(\"The series is NOT stationary (Fail to reject the null hypothesis)\")   numerical_features = [\"Temperature\", \"Pressure\", \"VibrationX\", \"VibrationY\", \"VibrationZ\", \"Frequency\"] for col in numerical_features:     s = df.sort_values('Cycle')[col]     test_stationarity(s, col) <pre>Dickey-Fuller (Dataset Temperature):\nADF statistic: -12.988680950011224\np-value: 2.8425779537946296e-24\nCritical values:\n\t1%: -3.438581476199162\n\t5%: -2.865173218890781\n\t10%: -2.56870466056054\nThe series is stationary (Reject the null hypothesis)\nDickey-Fuller (Dataset Pressure):\nADF statistic: -8.973295804703648\np-value: 7.6502921761507e-15\nCritical values:\n\t1%: -3.438602251755426\n\t5%: -2.8651823762743245\n\t10%: -2.5687095387840673\nThe series is stationary (Reject the null hypothesis)\nDickey-Fuller (Dataset VibrationX):\nADF statistic: -6.9958169063199565\np-value: 7.538878106337241e-10\nCritical values:\n\t1%: -3.438623132449471\n\t5%: -2.8651915799370014\n\t10%: -2.568714441670417\nThe series is stationary (Reject the null hypothesis)\nDickey-Fuller (Dataset VibrationY):\nADF statistic: -8.009749176554534\np-value: 2.2177469565403674e-12\nCritical values:\n\t1%: -3.438602251755426\n\t5%: -2.8651823762743245\n\t10%: -2.5687095387840673\nThe series is stationary (Reject the null hypothesis)\nDickey-Fuller (Dataset VibrationZ):\nADF statistic: -6.648510831277863\np-value: 5.191805426666831e-09\nCritical values:\n\t1%: -3.438633612472885\n\t5%: -2.865196199232788\n\t10%: -2.5687169024206713\nThe series is stationary (Reject the null hypothesis)\nDickey-Fuller (Dataset Frequency):\nADF statistic: -9.59674762354758\np-value: 1.9690415765210325e-16\nCritical values:\n\t1%: -3.438602251755426\n\t5%: -2.8651823762743245\n\t10%: -2.5687095387840673\nThe series is stationary (Reject the null hypothesis)\n</pre> In\u00a0[232]: Copied! <pre>def kde(df, numerical_columns, target_col):\n\n    num_cols = len(numerical_columns)\n    plt.figure(figsize=(18, 15))\n\n    for i, column in enumerate(numerical_columns):\n        plt.subplot(num_cols, 2, 2*i+1)\n        sns.kdeplot(data=df, x=column, hue=target_col, fill=True)\n        plt.title(f'KDE distribution of {column} by {target_col}')\n\n\n    plt.tight_layout()\n    plt.show()\n\nnumerical_features = [\"Temperature\", \"Pressure\", \"VibrationX\", \"VibrationY\", \"VibrationZ\", \"Frequency\"]\n\nkde(df, numerical_features, 'Fail')\n</pre> def kde(df, numerical_columns, target_col):      num_cols = len(numerical_columns)     plt.figure(figsize=(18, 15))      for i, column in enumerate(numerical_columns):         plt.subplot(num_cols, 2, 2*i+1)         sns.kdeplot(data=df, x=column, hue=target_col, fill=True)         plt.title(f'KDE distribution of {column} by {target_col}')       plt.tight_layout()     plt.show()  numerical_features = [\"Temperature\", \"Pressure\", \"VibrationX\", \"VibrationY\", \"VibrationZ\", \"Frequency\"]  kde(df, numerical_features, 'Fail')  In\u00a0[233]: Copied! <pre>def plot_correlation_heatmap(df,features):\n    corr_matrix = df[features].corr()\n\n    plt.figure(figsize=(8, 3))\n\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n\n    plt.title('Heatmap of numerical features correlation')\n    plt.show()\n\nplot_correlation_heatmap(df,numerical_features)\n</pre> def plot_correlation_heatmap(df,features):     corr_matrix = df[features].corr()      plt.figure(figsize=(8, 3))      sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)      plt.title('Heatmap of numerical features correlation')     plt.show()  plot_correlation_heatmap(df,numerical_features) In\u00a0[234]: Copied! <pre>def preprocess(data,drops, columns, target):\n  data_copy = data.copy()\n  y = data_copy[target]\n  X = data_copy.drop(drops, axis=1)\n\n  scaler = StandardScaler()\n  X[columns] = scaler.fit_transform(X[columns])\n\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n  return  X_train, X_test, y_train, y_test\n\ndef train_model(X_train, y_train, model_exp):\n\n    scorer = make_scorer(recall_score, pos_label=1)\n\n    if model_exp == 'Random Forest':\n        model = RandomForestClassifier(random_state=42)\n        param_grid = {\n            'n_estimators': [100, 200, 300],\n            'max_depth': [10, 20, None],\n            'min_samples_split': [2, 5, 10]\n        }\n\n    elif model_exp == 'SVM':\n        model = SVC(kernel='rbf', random_state=42)\n        param_grid = {\n            'C': [0.1, 1, 10],\n            'gamma': ['scale', 'auto'],\n            'kernel': ['rbf']\n        }\n\n    elif model_exp == 'Logistic Regression':\n        model = LogisticRegression(max_iter=1000, solver='saga', n_jobs=-1)\n        param_grid = {\n            'C': [0.01, 0.1, 1, 10],\n            'penalty': ['l1', 'l2']\n        }\n\n    elif model_exp == 'Extra Trees':\n        model = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n        param_grid = {\n            'n_estimators': [200, 400],\n            'max_depth': [None, 10, 20],\n            'min_samples_split': [2, 10]\n        }\n\n    elif model_exp == 'Gradient Boosting':\n        model = GradientBoostingClassifier(random_state=42)\n        param_grid = {\n            'n_estimators': [100, 300],\n            'learning_rate': [0.05, 0.1],\n            'max_depth': [2, 3]\n        }\n\n    elif model_exp == 'HistGB':\n        model = HistGradientBoostingClassifier(random_state=42)\n        param_grid = {\n            'max_depth': [None, 6, 12],\n            'learning_rate': [0.05, 0.1],\n            'max_leaf_nodes': [31, 63]\n        }\n\n    elif model_exp == 'KNN':\n        model = KNeighborsClassifier()\n        param_grid = {\n            'n_neighbors': [5, 15, 30],\n            'weights': ['uniform', 'distance'],\n            'p': [1, 2]\n        }\n\n    elif model_exp == 'Linear SVM':\n        model = LinearSVC(random_state=42)\n        param_grid = {\n            'C': [0.1, 1, 10]\n        }\n\n    grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n                               scoring=scorer, cv=5, n_jobs=-1, verbose=2)\n    grid_search.fit(X_train, y_train)\n\n    print(f\"Better parameters for {model_exp}: {grid_search.best_params_}\")\n    return grid_search.best_estimator_\n\n\ndef predict_and_evaluate(model, X_test, y_test):\n\n  y_pred = model.predict(X_test)\n  conf_matrix = confusion_matrix(y_test, y_pred)\n  sns.heatmap(conf_matrix, annot=True, fmt='d')\n  plt.title('Confusion Matrix')\n  plt.show()\n  print(f'Model: {model}')\n  print(classification_report(y_test, y_pred))\n\n  f1 = f1_score(y_test, y_pred)\n  print(f'F1 score: {f1 * 100:.2f}%')\n</pre> def preprocess(data,drops, columns, target):   data_copy = data.copy()   y = data_copy[target]   X = data_copy.drop(drops, axis=1)    scaler = StandardScaler()   X[columns] = scaler.fit_transform(X[columns])    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)   return  X_train, X_test, y_train, y_test  def train_model(X_train, y_train, model_exp):      scorer = make_scorer(recall_score, pos_label=1)      if model_exp == 'Random Forest':         model = RandomForestClassifier(random_state=42)         param_grid = {             'n_estimators': [100, 200, 300],             'max_depth': [10, 20, None],             'min_samples_split': [2, 5, 10]         }      elif model_exp == 'SVM':         model = SVC(kernel='rbf', random_state=42)         param_grid = {             'C': [0.1, 1, 10],             'gamma': ['scale', 'auto'],             'kernel': ['rbf']         }      elif model_exp == 'Logistic Regression':         model = LogisticRegression(max_iter=1000, solver='saga', n_jobs=-1)         param_grid = {             'C': [0.01, 0.1, 1, 10],             'penalty': ['l1', 'l2']         }      elif model_exp == 'Extra Trees':         model = ExtraTreesClassifier(random_state=42, n_jobs=-1)         param_grid = {             'n_estimators': [200, 400],             'max_depth': [None, 10, 20],             'min_samples_split': [2, 10]         }      elif model_exp == 'Gradient Boosting':         model = GradientBoostingClassifier(random_state=42)         param_grid = {             'n_estimators': [100, 300],             'learning_rate': [0.05, 0.1],             'max_depth': [2, 3]         }      elif model_exp == 'HistGB':         model = HistGradientBoostingClassifier(random_state=42)         param_grid = {             'max_depth': [None, 6, 12],             'learning_rate': [0.05, 0.1],             'max_leaf_nodes': [31, 63]         }      elif model_exp == 'KNN':         model = KNeighborsClassifier()         param_grid = {             'n_neighbors': [5, 15, 30],             'weights': ['uniform', 'distance'],             'p': [1, 2]         }      elif model_exp == 'Linear SVM':         model = LinearSVC(random_state=42)         param_grid = {             'C': [0.1, 1, 10]         }      grid_search = GridSearchCV(estimator=model, param_grid=param_grid,                                scoring=scorer, cv=5, n_jobs=-1, verbose=2)     grid_search.fit(X_train, y_train)      print(f\"Better parameters for {model_exp}: {grid_search.best_params_}\")     return grid_search.best_estimator_   def predict_and_evaluate(model, X_test, y_test):    y_pred = model.predict(X_test)   conf_matrix = confusion_matrix(y_test, y_pred)   sns.heatmap(conf_matrix, annot=True, fmt='d')   plt.title('Confusion Matrix')   plt.show()   print(f'Model: {model}')   print(classification_report(y_test, y_pred))    f1 = f1_score(y_test, y_pred)   print(f'F1 score: {f1 * 100:.2f}%') In\u00a0[235]: Copied! <pre>df[\"Preset_1\"] = df[\"Preset_1\"].astype(str)\ndf[\"Preset_2\"] = df[\"Preset_2\"].astype(str)\ndf_hot = pd.get_dummies(df, columns=[\"Preset_1\", \"Preset_2\"], drop_first=False)\n</pre> df[\"Preset_1\"] = df[\"Preset_1\"].astype(str) df[\"Preset_2\"] = df[\"Preset_2\"].astype(str) df_hot = pd.get_dummies(df, columns=[\"Preset_1\", \"Preset_2\"], drop_first=False) In\u00a0[236]: Copied! <pre>models = ['Random Forest', 'SVM', 'Logistic Regression', 'Extra Trees', 'Gradient Boosting', 'HistGB', 'KNN', 'Linear SVM']\naux = df.copy()\naux[\"Fail\"] = aux[\"Fail\"].apply(lambda x: 1 if x else 0)\ndrops = [\"Preset_1\",  \"Preset_2\", \"Fail\", \"Cycle\"]\nX_train, X_test, y_train, y_test = preprocess(aux, drops, numerical_features, \"Fail\")\nros = RandomOverSampler(sampling_strategy=0.5, random_state=42)\nX_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\nX_train_hot, X_test_hot, y_train_hot, y_test_hot = preprocess(df_hot, [\"Fail\", \"Cycle\"], numerical_features, \"Fail\")\nX_train_hot_ros, y_train_hot_ros = ros.fit_resample(X_train_hot, y_train_hot)\n\nfor model in models:\n  print('-------------------------------------------------------------------------------')\n  print('-------------------------- Original dataset -----------------------------------')\n  print(f'Model: {model}')\n  print('-------------------------------------------------------------------------------')\n  tmodel = train_model(X_train, y_train, model)\n  predict_and_evaluate(tmodel, X_test, y_test)\n  print('-------------------------------------------------------------------------------')\n  print('-------------------------- Random oversampled ---------------------------------')\n  print(f'Model: {model}')\n  print('-------------------------------------------------------------------------------')\n  tmodel = train_model(X_train_ros, y_train_ros, model)\n  predict_and_evaluate(tmodel, X_test, y_test)\n  print('-------------------------------------------------------------------------------')\n  print('-------------------------- Original dataset one hot encoded -------------------')\n  print(f'Model: {model}')\n  print('-------------------------------------------------------------------------------')\n  tmodel = train_model(X_train_hot, y_train_hot, model)\n  predict_and_evaluate(tmodel, X_test_hot, y_test_hot)\n  print('-------------------------------------------------------------------------------')\n  print('-------------------------- Random oversampled one hot encoded -----------------')\n  print(f'Model: {model}')\n  print('-------------------------------------------------------------------------------')\n  tmodel = train_model(X_train_hot_ros, y_train_hot_ros, model)\n  predict_and_evaluate(tmodel, X_test_hot, y_test_hot)\n  print('-------------------------------------------------------------------------------')\n</pre> models = ['Random Forest', 'SVM', 'Logistic Regression', 'Extra Trees', 'Gradient Boosting', 'HistGB', 'KNN', 'Linear SVM'] aux = df.copy() aux[\"Fail\"] = aux[\"Fail\"].apply(lambda x: 1 if x else 0) drops = [\"Preset_1\",  \"Preset_2\", \"Fail\", \"Cycle\"] X_train, X_test, y_train, y_test = preprocess(aux, drops, numerical_features, \"Fail\") ros = RandomOverSampler(sampling_strategy=0.5, random_state=42) X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train) X_train_hot, X_test_hot, y_train_hot, y_test_hot = preprocess(df_hot, [\"Fail\", \"Cycle\"], numerical_features, \"Fail\") X_train_hot_ros, y_train_hot_ros = ros.fit_resample(X_train_hot, y_train_hot)  for model in models:   print('-------------------------------------------------------------------------------')   print('-------------------------- Original dataset -----------------------------------')   print(f'Model: {model}')   print('-------------------------------------------------------------------------------')   tmodel = train_model(X_train, y_train, model)   predict_and_evaluate(tmodel, X_test, y_test)   print('-------------------------------------------------------------------------------')   print('-------------------------- Random oversampled ---------------------------------')   print(f'Model: {model}')   print('-------------------------------------------------------------------------------')   tmodel = train_model(X_train_ros, y_train_ros, model)   predict_and_evaluate(tmodel, X_test, y_test)   print('-------------------------------------------------------------------------------')   print('-------------------------- Original dataset one hot encoded -------------------')   print(f'Model: {model}')   print('-------------------------------------------------------------------------------')   tmodel = train_model(X_train_hot, y_train_hot, model)   predict_and_evaluate(tmodel, X_test_hot, y_test_hot)   print('-------------------------------------------------------------------------------')   print('-------------------------- Random oversampled one hot encoded -----------------')   print(f'Model: {model}')   print('-------------------------------------------------------------------------------')   tmodel = train_model(X_train_hot_ros, y_train_hot_ros, model)   predict_and_evaluate(tmodel, X_test_hot, y_test_hot)   print('-------------------------------------------------------------------------------') <pre>-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: Random Forest\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 27 candidates, totalling 135 fits\n</pre> <pre>[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\nBetter parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n</pre> <pre>Model: RandomForestClassifier(max_depth=10, n_estimators=200, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.97      0.99      0.98       225\n           1       0.70      0.47      0.56        15\n\n    accuracy                           0.95       240\n   macro avg       0.83      0.73      0.77       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 56.00%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: Random Forest\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 27 candidates, totalling 135 fits\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\nBetter parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n</pre> <pre>Model: RandomForestClassifier(max_depth=10, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97       225\n           1       0.59      0.67      0.62        15\n\n    accuracy                           0.95       240\n   macro avg       0.78      0.82      0.80       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 62.50%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: Random Forest\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 27 candidates, totalling 135 fits\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.4s\n\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.3s\nBetter parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n</pre> <pre>Model: RandomForestClassifier(max_depth=10, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.97      0.99      0.98       225\n        True       0.78      0.47      0.58        15\n\n    accuracy                           0.96       240\n   macro avg       0.87      0.73      0.78       240\nweighted avg       0.95      0.96      0.95       240\n\nF1 score: 58.33%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: Random Forest\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 27 candidates, totalling 135 fits\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   0.3s\nBetter parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n</pre> <pre>Model: RandomForestClassifier(max_depth=10, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.97      0.98      0.97       225\n        True       0.62      0.53      0.57        15\n\n    accuracy                           0.95       240\n   macro avg       0.79      0.76      0.77       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 57.14%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\nBetter parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n</pre> <pre>Model: SVC(C=10, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98       225\n           1       0.63      0.80      0.71        15\n\n    accuracy                           0.96       240\n   macro avg       0.81      0.88      0.84       240\nweighted avg       0.96      0.96      0.96       240\n\nF1 score: 70.59%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\nBetter parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n</pre> <pre>Model: SVC(C=10, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97       225\n           1       0.54      0.87      0.67        15\n\n    accuracy                           0.95       240\n   macro avg       0.77      0.91      0.82       240\nweighted avg       0.96      0.95      0.95       240\n\nF1 score: 66.67%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\nBetter parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n</pre> <pre>Model: SVC(C=10, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.96      0.97      0.97       225\n        True       0.54      0.47      0.50        15\n\n    accuracy                           0.94       240\n   macro avg       0.75      0.72      0.73       240\nweighted avg       0.94      0.94      0.94       240\n\nF1 score: 50.00%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\nBetter parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n</pre> <pre>Model: SVC(C=10, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.96      0.97      0.97       225\n        True       0.54      0.47      0.50        15\n\n    accuracy                           0.94       240\n   macro avg       0.75      0.72      0.73       240\nweighted avg       0.94      0.94      0.94       240\n\nF1 score: 50.00%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: Logistic Regression\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\nBetter parameters for Logistic Regression: {'C': 1, 'penalty': 'l1'}\n</pre> <pre>Model: LogisticRegression(C=1, max_iter=1000, n_jobs=-1, penalty='l1', solver='saga')\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97       225\n           1       0.60      0.60      0.60        15\n\n    accuracy                           0.95       240\n   macro avg       0.79      0.79      0.79       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 60.00%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: Logistic Regression\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n[CV] END ...................................C=10, penalty=l2; total time=   0.0s\nBetter parameters for Logistic Regression: {'C': 0.1, 'penalty': 'l2'}\n</pre> <pre>Model: LogisticRegression(C=0.1, max_iter=1000, n_jobs=-1, solver='saga')\n              precision    recall  f1-score   support\n\n           0       1.00      0.94      0.97       225\n           1       0.52      1.00      0.68        15\n\n    accuracy                           0.94       240\n   macro avg       0.76      0.97      0.82       240\nweighted avg       0.97      0.94      0.95       240\n\nF1 score: 68.18%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: Logistic Regression\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n</pre> <pre>/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning:\n\nThe max_iter was reached which means the coef_ did not converge\n\n</pre> <pre>[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\nBetter parameters for Logistic Regression: {'C': 10, 'penalty': 'l1'}\n</pre> <pre>Model: LogisticRegression(C=10, max_iter=1000, n_jobs=-1, penalty='l1', solver='saga')\n              precision    recall  f1-score   support\n\n       False       0.97      0.97      0.97       225\n        True       0.56      0.60      0.58        15\n\n    accuracy                           0.95       240\n   macro avg       0.77      0.78      0.78       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 58.06%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: Logistic Regression\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n[CV] END ..................................C=0.1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n[CV] END ...................................C=10, penalty=l2; total time=   0.2s\n[CV] END ...................................C=10, penalty=l2; total time=   0.2s\nBetter parameters for Logistic Regression: {'C': 0.1, 'penalty': 'l2'}\n</pre> <pre>/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/ds_test_docs_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n</pre> <pre>Model: LogisticRegression(C=0.1, max_iter=1000, n_jobs=-1, solver='saga')\n              precision    recall  f1-score   support\n\n       False       1.00      0.94      0.97       225\n        True       0.54      1.00      0.70        15\n\n    accuracy                           0.95       240\n   macro avg       0.77      0.97      0.83       240\nweighted avg       0.97      0.95      0.95       240\n\nF1 score: 69.77%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: Extra Trees\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.4s\nBetter parameters for Extra Trees: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n</pre> <pre>Model: ExtraTreesClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.96      0.99      0.97       225\n           1       0.62      0.33      0.43        15\n\n    accuracy                           0.95       240\n   macro avg       0.79      0.66      0.70       240\nweighted avg       0.94      0.95      0.94       240\n\nF1 score: 43.48%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: Extra Trees\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\nBetter parameters for Extra Trees: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n</pre> <pre>Model: ExtraTreesClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.97      0.99      0.98       225\n           1       0.70      0.47      0.56        15\n\n    accuracy                           0.95       240\n   macro avg       0.83      0.73      0.77       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 56.00%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: Extra Trees\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.8s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.8s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.4s\nBetter parameters for Extra Trees: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n</pre> <pre>Model: ExtraTreesClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.96      1.00      0.98       225\n        True       0.86      0.40      0.55        15\n\n    accuracy                           0.96       240\n   macro avg       0.91      0.70      0.76       240\nweighted avg       0.95      0.96      0.95       240\n\nF1 score: 54.55%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: Extra Trees\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   0.9s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   1.0s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=400; total time=   1.0s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=400; total time=   0.9s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.9s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=400; total time=   0.8s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.8s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.8s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=400; total time=   0.7s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.6s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=400; total time=   0.5s\nBetter parameters for Extra Trees: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n</pre> <pre>Model: ExtraTreesClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.96      1.00      0.98       225\n        True       0.83      0.33      0.48        15\n\n    accuracy                           0.95       240\n   macro avg       0.90      0.66      0.73       240\nweighted avg       0.95      0.95      0.94       240\n\nF1 score: 47.62%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: Gradient Boosting\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.1s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.1s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.1s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\nBetter parameters for Gradient Boosting: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300}\n</pre> <pre>Model: GradientBoostingClassifier(learning_rate=0.05, max_depth=2, n_estimators=300,\n                           random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97       225\n           1       0.53      0.53      0.53        15\n\n    accuracy                           0.94       240\n   macro avg       0.75      0.75      0.75       240\nweighted avg       0.94      0.94      0.94       240\n\nF1 score: 53.33%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: Gradient Boosting\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.1s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.1s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.6s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.6s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\nBetter parameters for Gradient Boosting: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300}\n</pre> <pre>Model: GradientBoostingClassifier(learning_rate=0.05, max_depth=2, n_estimators=300,\n                           random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       225\n           1       0.56      0.67      0.61        15\n\n    accuracy                           0.95       240\n   macro avg       0.77      0.82      0.79       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 60.61%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: Gradient Boosting\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.1s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.6s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\nBetter parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}\n</pre> <pre>Model: GradientBoostingClassifier(n_estimators=300, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.97      0.98      0.98       225\n        True       0.67      0.53      0.59        15\n\n    accuracy                           0.95       240\n   macro avg       0.82      0.76      0.78       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 59.26%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: Gradient Boosting\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.3s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=2, n_estimators=300; total time=   0.6s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.3s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.7s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.4s\n[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.6s[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\nBetter parameters for Gradient Boosting: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300}\n</pre> <pre>Model: GradientBoostingClassifier(learning_rate=0.05, max_depth=2, n_estimators=300,\n                           random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.97      0.96      0.97       225\n        True       0.53      0.60      0.56        15\n\n    accuracy                           0.94       240\n   macro avg       0.75      0.78      0.77       240\nweighted avg       0.95      0.94      0.94       240\n\nF1 score: 56.25%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: HistGB\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\nBetter parameters for HistGB: {'learning_rate': 0.1, 'max_depth': None, 'max_leaf_nodes': 31}\n</pre> <pre>Model: HistGradientBoostingClassifier(random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97       225\n           1       0.60      0.60      0.60        15\n\n    accuracy                           0.95       240\n   macro avg       0.79      0.79      0.79       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 60.00%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: HistGB\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.3s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.3s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\nBetter parameters for HistGB: {'learning_rate': 0.05, 'max_depth': None, 'max_leaf_nodes': 31}\n</pre> <pre>Model: HistGradientBoostingClassifier(learning_rate=0.05, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.98       225\n           1       0.60      0.80      0.69        15\n\n    accuracy                           0.95       240\n   macro avg       0.79      0.88      0.83       240\nweighted avg       0.96      0.95      0.96       240\n\nF1 score: 68.57%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: HistGB\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.3s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\nBetter parameters for HistGB: {'learning_rate': 0.1, 'max_depth': None, 'max_leaf_nodes': 31}\n</pre> <pre>Model: HistGradientBoostingClassifier(random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.98      0.97      0.97       225\n        True       0.59      0.67      0.62        15\n\n    accuracy                           0.95       240\n   macro avg       0.78      0.82      0.80       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 62.50%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: HistGB\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.05, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.05, max_depth=12, max_leaf_nodes=63; total time=   0.3s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.3s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=31; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=None, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=31; total time=   0.2s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.1s\n[CV] END ..learning_rate=0.1, max_depth=6, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.1s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=63; total time=   0.2s\n[CV] END .learning_rate=0.1, max_depth=12, max_leaf_nodes=31; total time=   0.2s\nBetter parameters for HistGB: {'learning_rate': 0.05, 'max_depth': None, 'max_leaf_nodes': 31}\n</pre> <pre>Model: HistGradientBoostingClassifier(learning_rate=0.05, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.98      0.97      0.98       225\n        True       0.61      0.73      0.67        15\n\n    accuracy                           0.95       240\n   macro avg       0.80      0.85      0.82       240\nweighted avg       0.96      0.95      0.96       240\n\nF1 score: 66.67%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: KNN\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\nBetter parameters for KNN: {'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n</pre> <pre>Model: KNeighborsClassifier()\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96       225\n           1       0.43      0.40      0.41        15\n\n    accuracy                           0.93       240\n   macro avg       0.69      0.68      0.69       240\nweighted avg       0.93      0.93      0.93       240\n\nF1 score: 41.38%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: KNN\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\nBetter parameters for KNN: {'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n</pre> <pre>Model: KNeighborsClassifier(p=1, weights='distance')\n              precision    recall  f1-score   support\n\n           0       0.99      0.92      0.95       225\n           1       0.41      0.80      0.55        15\n\n    accuracy                           0.92       240\n   macro avg       0.70      0.86      0.75       240\nweighted avg       0.95      0.92      0.93       240\n\nF1 score: 54.55%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: KNN\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.1s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.1s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.1s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.1s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\nBetter parameters for KNN: {'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n</pre> <pre>Model: KNeighborsClassifier()\n              precision    recall  f1-score   support\n\n       False       0.97      0.96      0.97       225\n        True       0.50      0.53      0.52        15\n\n    accuracy                           0.94       240\n   macro avg       0.73      0.75      0.74       240\nweighted avg       0.94      0.94      0.94       240\n\nF1 score: 51.61%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: KNN\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=1, weights=distance; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=1, weights=uniform; total time=   0.0s\n[CV] END ................n_neighbors=5, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=5, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=15, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=15, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=1, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=1, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\n[CV] END ..............n_neighbors=30, p=2, weights=distance; total time=   0.0s\n[CV] END ...............n_neighbors=30, p=2, weights=uniform; total time=   0.0s\nBetter parameters for KNN: {'n_neighbors': 5, 'p': 2, 'weights': 'distance'}\n</pre> <pre>Model: KNeighborsClassifier(weights='distance')\n              precision    recall  f1-score   support\n\n       False       0.99      0.91      0.95       225\n        True       0.39      0.87      0.54        15\n\n    accuracy                           0.91       240\n   macro avg       0.69      0.89      0.75       240\nweighted avg       0.95      0.91      0.92       240\n\nF1 score: 54.17%\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n-------------------------- Original dataset -----------------------------------\nModel: Linear SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\nBetter parameters for Linear SVM: {'C': 10}\n</pre> <pre>Model: LinearSVC(C=10, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97       225\n           1       0.60      0.60      0.60        15\n\n    accuracy                           0.95       240\n   macro avg       0.79      0.79      0.79       240\nweighted avg       0.95      0.95      0.95       240\n\nF1 score: 60.00%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled ---------------------------------\nModel: Linear SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\nBetter parameters for Linear SVM: {'C': 0.1}\n</pre> <pre>Model: LinearSVC(C=0.1, random_state=42)\n              precision    recall  f1-score   support\n\n           0       1.00      0.93      0.97       225\n           1       0.50      1.00      0.67        15\n\n    accuracy                           0.94       240\n   macro avg       0.75      0.97      0.82       240\nweighted avg       0.97      0.94      0.95       240\n\nF1 score: 66.67%\n-------------------------------------------------------------------------------\n-------------------------- Original dataset one hot encoded -------------------\nModel: Linear SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\nBetter parameters for Linear SVM: {'C': 1}\n</pre> <pre>Model: LinearSVC(C=1, random_state=42)\n              precision    recall  f1-score   support\n\n       False       0.97      0.97      0.97       225\n        True       0.57      0.53      0.55        15\n\n    accuracy                           0.95       240\n   macro avg       0.77      0.75      0.76       240\nweighted avg       0.94      0.95      0.94       240\n\nF1 score: 55.17%\n-------------------------------------------------------------------------------\n-------------------------- Random oversampled one hot encoded -----------------\nModel: Linear SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ................................................C=1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ..............................................C=0.1; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\n[CV] END ...............................................C=10; total time=   0.0s\nBetter parameters for Linear SVM: {'C': 0.1}\n</pre> <pre>Model: LinearSVC(C=0.1, random_state=42)\n              precision    recall  f1-score   support\n\n       False       1.00      0.93      0.96       225\n        True       0.48      1.00      0.65        15\n\n    accuracy                           0.93       240\n   macro avg       0.74      0.96      0.81       240\nweighted avg       0.97      0.93      0.94       240\n\nF1 score: 65.22%\n-------------------------------------------------------------------------------\n</pre> In\u00a0[216]: Copied! <pre>model = \"SVM\"\n\nprint('-------------------------- Original dataset -----------------------------------')\nprint(f'Model: {model}')\nprint('-------------------------------------------------------------------------------')\nX_train, X_test, y_train, y_test = preprocess(aux, drops, numerical_features, \"Fail\")\ntmodel = train_model(X_train, y_train, model)\npredict_and_evaluate(tmodel, X_test, y_test)\n</pre> model = \"SVM\"  print('-------------------------- Original dataset -----------------------------------') print(f'Model: {model}') print('-------------------------------------------------------------------------------') X_train, X_test, y_train, y_test = preprocess(aux, drops, numerical_features, \"Fail\") tmodel = train_model(X_train, y_train, model) predict_and_evaluate(tmodel, X_test, y_test) <pre>-------------------------- Original dataset -----------------------------------\nModel: SVM\n-------------------------------------------------------------------------------\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\nBetter parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n</pre> <pre>Model: SVC(C=10, random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98       225\n           1       0.63      0.80      0.71        15\n\n    accuracy                           0.96       240\n   macro avg       0.81      0.88      0.84       240\nweighted avg       0.96      0.96      0.96       240\n\nF1 score: 70.59%\n</pre> In\u00a0[219]: Copied! <pre>X_train, X_test, y_train, y_test = preprocess(aux, drops, numerical_features, \"Fail\")\n\ntmodel = RandomForestClassifier(max_depth=10, min_samples_split=10, \n                                n_estimators=300, random_state=42)\n\ntmodel.fit(X_train, y_train)\n\npredict_and_evaluate(tmodel, X_test, y_test)\n\ndef plot_feature_importance(model, X_train):\n\n    importances = model.feature_importances_\n\n    feature_names = X_train.columns\n\n    feature_importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances\n    }).sort_values(by='Importance', ascending=False)\n\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='royalblue')\n    plt.xlabel('Importance')\n    plt.ylabel('Feature')\n    plt.title('Feature importance - Random Forest')\n    plt.gca().invert_yaxis()\n    plt.show()\n\nplot_feature_importance(tmodel, X_train)\n</pre> X_train, X_test, y_train, y_test = preprocess(aux, drops, numerical_features, \"Fail\")  tmodel = RandomForestClassifier(max_depth=10, min_samples_split=10,                                  n_estimators=300, random_state=42)  tmodel.fit(X_train, y_train)  predict_and_evaluate(tmodel, X_test, y_test)  def plot_feature_importance(model, X_train):      importances = model.feature_importances_      feature_names = X_train.columns      feature_importance_df = pd.DataFrame({         'Feature': feature_names,         'Importance': importances     }).sort_values(by='Importance', ascending=False)      plt.figure(figsize=(10, 6))     plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='royalblue')     plt.xlabel('Importance')     plt.ylabel('Feature')     plt.title('Feature importance - Random Forest')     plt.gca().invert_yaxis()     plt.show()  plot_feature_importance(tmodel, X_train)  <pre>Model: RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=300,\n                       random_state=42)\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97       225\n           1       0.64      0.47      0.54        15\n\n    accuracy                           0.95       240\n   macro avg       0.80      0.72      0.76       240\nweighted avg       0.94      0.95      0.95       240\n\nF1 score: 53.85%\n</pre>"},{"location":"answers/#major-task","title":"Major task\u00b6","text":"<ul> <li>Investigate one piece of equipment in different time cycles to understand what characteristics and parameters of the sensors might indicate that the equipment is on the verge of failing.</li> </ul>"},{"location":"answers/#task-1","title":"Task 1\u00b6","text":"<ul> <li>Calculate how many times the equipment has failed.</li> </ul> <p>During the FPSO\u2019s operation, various factors can cause the machine to fail and prolong its failure state.</p> <p>We ask you to explore the available data, identify, and calculate the number of times the equipment has failed throughout its operation.</p>"},{"location":"answers/#task-2","title":"Task 2\u00b6","text":"<ul> <li><p>Categorize equipment failures by setup configurations (Preset 1 and Preset 2).</p> </li> <li><p>How do the variables Preset_1 and Preset_2 behave during operation?</p> </li> <li><p>What insights can we derive from these variables?</p> </li> </ul>"},{"location":"answers/#evaluating-presets-regarding-number-of-obsertations-and-failure-percentage","title":"Evaluating Presets regarding Number of obsertations and Failure percentage\u00b6","text":""},{"location":"answers/#presets-failure-percentage-heatmap","title":"Presets Failure Percentage Heatmap\u00b6","text":""},{"location":"answers/#presets-state-changes-during-operation","title":"Presets state changes during operation\u00b6","text":""},{"location":"answers/#boxplots-per-presets-during-operation","title":"Boxplots per presets during operation\u00b6","text":""},{"location":"answers/#task-3","title":"Task 3\u00b6","text":"<ul> <li><p>Categorize equipment failures by their nature/root cause according to parameter readings (temperature, pressure, and others).</p> </li> <li><p>Analyze patterns in these readings that could indicate specific failure types.</p> </li> <li><p>How do these patterns differ across operational regimes?</p> </li> <li><p>Provide insights based on your findings.</p> </li> </ul>"},{"location":"answers/#task-4","title":"Task 4\u00b6","text":"<ul> <li><p>Create a model (or models) using the technique you think is most appropriate and measure its performance.</p> </li> <li><p>Based on the given time-series dataset, which models or techniques are suitable for predicting whether the equipment will fail before it occurs?</p> </li> <li><p>Additionally, how can the model's performance be tuned and measured for this task?</p> </li> </ul>"},{"location":"answers/#model-choice","title":"Model choice\u00b6","text":""},{"location":"answers/#task-5","title":"Task 5\u00b6","text":"<ul> <li><p>Analyze variable importance.</p> </li> <li><p>After developing a model, how can we determine which variables had the greatest impact on the prediction?</p> </li> </ul>"}]}